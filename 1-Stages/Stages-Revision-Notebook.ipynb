{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b1963a",
   "metadata": {},
   "source": [
    "\n",
    "# Comprehensive Revision Notebook\n",
    "This notebook serves as a comprehensive revision of the key concepts learned across all stages of the course, including:\n",
    "- Loading and Chunking\n",
    "- Tokenization\n",
    "- Embeddings and Vector Representations\n",
    "- Vector Databases (ChromaDB)\n",
    "- Indexing (VectorStoreIndex, Query Engines, Retrievers)\n",
    "- Response Synthesizers (Refine, Compact, Tree Summarize, Accumulate, Compact Accumulate)\n",
    "\n",
    "We will demonstrate the integration and application of all these components in a unified workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required libraries\n",
    "%pip install chromadb llama-index llama-index-vector-stores-chroma\n",
    "\n",
    "# Import necessary libraries\n",
    "import chromadb\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Stage 1: Loading and Chunking**\n",
    "\n",
    "# Load a sample document (adjust file path as necessary)\n",
    "documents = SimpleDirectoryReader(input_files=['../data_uber/uber_2021.pdf']).load_data(show_progress=True)\n",
    "\n",
    "# Display the first document\n",
    "print(\"First Document Content:\")\n",
    "print(documents[0].text[:500])  # Show first 500 characters\n",
    "print(\"Length of the documents:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Stage 2: Embeddings and Vector Representations**\n",
    "\n",
    "# Initialize the embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "# Generate embeddings for the loaded documents\n",
    "embeddings = [ollama_embedding.get_text_embedding(doc.text) for doc in documents]\n",
    "\n",
    "# Display embedding dimensionality\n",
    "print(f\"Embedding Dimensionality: {len(embeddings[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb54e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Stage 3: Indexing with ChromaDB**\n",
    "\n",
    "# Initialize ChromaDB Persistent Client\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create or retrieve a collection in ChromaDB\n",
    "chroma_collection = db.get_or_create_collection(\"revision_collection\")\n",
    "\n",
    "# Set up Chroma as the vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Build a VectorStoreIndex using the embeddings\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embed_model=ollama_embedding)\n",
    "\n",
    "# Save the index for reuse\n",
    "index.storage_context.vector_store.persist(\"revision_vector_store.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Stage 4: Querying with Query Engines and Retrievers**\n",
    "\n",
    "# Configure a retriever for similarity-based querying\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "query = \"Summarize the main points of the document.\"\n",
    "retrieved_nodes = retriever.retrieve(query)\n",
    "\n",
    "# Display retrieved nodes\n",
    "for i, node in enumerate(retrieved_nodes, start=1):\n",
    "    print(f\"Node {i} Content:\")\n",
    "    print(node.get_content())\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b73ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import  Ollama\n",
    "\n",
    "Settings.llm = Ollama(model='llama3.2:latest', base_url='http://localhost:11434',temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6debdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Stage 5: Response Synthesizers**\n",
    "\n",
    "# Refine Mode\n",
    "refine_synthesizer = get_response_synthesizer(response_mode=\"refine\")\n",
    "refine_response = index.as_query_engine(response_synthesizer=refine_synthesizer).query(query)\n",
    "print(\"Refine Mode Response:\")\n",
    "print(\"=====================\")\n",
    "print(refine_response)\n",
    "\n",
    "# Compact Mode\n",
    "compact_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "compact_response = index.as_query_engine(response_synthesizer=compact_synthesizer).query(query)\n",
    "print(\"Compact Mode Response:\")\n",
    "print(\"======================\")\n",
    "print(compact_response)\n",
    "\n",
    "# Tree Summarize Mode\n",
    "tree_summarize_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
    "tree_response = index.as_query_engine(response_synthesizer=tree_summarize_synthesizer).query(query)\n",
    "print(\"Tree Summarize Response:\")\n",
    "print(\"========================\")\n",
    "print(tree_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a9b8dc",
   "metadata": {},
   "source": [
    "\n",
    "# **Conclusion**\n",
    "\n",
    "This notebook demonstrates the integration of all key concepts covered in the course. By combining stages such as loading, embedding, indexing, querying, and synthesizing, we have created a robust pipeline for managing and querying textual data efficiently.\n",
    "\n",
    "You can extend this workflow further by experimenting with different embedding models, vector databases, or custom query logic. Happy learning!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
