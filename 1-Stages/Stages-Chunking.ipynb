{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter, TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=['../data/2022 Q3 AAPL.pdf']).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract all metadata into a list of dictionaries\n",
    "# Assuming each `node` object has a `metadata` attribute\n",
    "metadata_list = [node.metadata for node in nodes]\n",
    "\n",
    "# Create a pandas DataFrame from the metadata list\n",
    "metadata_df = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(metadata_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "import tiktoken\n",
    "\n",
    "# Load the Hugging Face LLaMA tokenizer\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(\"huggingface/llama-model\")\n",
    "\n",
    "# Load OpenAI's GPT tokenizer\n",
    "gpt_encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Loop through the nodes and calculate token counts for both models\n",
    "for i, node in enumerate(nodes):\n",
    "    # Tokenize with GPT encoding\n",
    "    gpt_token_count = len(gpt_encoding.encode(node.text))\n",
    "\n",
    "    # Tokenize with LLaMA encoding\n",
    "    llama_tokens = llama_tokenizer.encode(node.text, add_special_tokens=False)\n",
    "    llama_token_count = len(llama_tokens)\n",
    "\n",
    "    print(f'Number of GPT tokens in node {i+1}: {gpt_token_count}')\n",
    "    print(f'Number of LLaMA tokens in node {i+1}: {llama_token_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "import tiktoken\n",
    "\n",
    "# Load the Hugging Face LLaMA tokenizer\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(\"huggingface/llama-model\")\n",
    "\n",
    "# Load OpenAI's GPT tokenizer\n",
    "gpt_encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Loop through the nodes and calculate token counts for both models\n",
    "for i, node in enumerate(nodes):\n",
    "    # Tokenize with GPT encoding\n",
    "    gpt_token_count = len(gpt_encoding.encode(node.text))\n",
    "\n",
    "    # Tokenize with LLaMA encoding\n",
    "    llama_tokens = llama_tokenizer.encode(node.text, add_special_tokens=False)\n",
    "    llama_token_count = len(llama_tokens)\n",
    "\n",
    "    print(f'Number of GPT tokens in node {i+1}: {gpt_token_count}')\n",
    "    print(f'Number of LLaMA tokens in node {i+1}: {llama_token_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.encode(nodes[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
