{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Should display your GPU's name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-extractors-entity in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: huggingface-hub<0.24.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-extractors-entity) (0.23.5)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-extractors-entity) (0.12.1)\n",
      "Requirement already satisfied: span-marker>=1.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-extractors-entity) (1.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<0.24.0->llama-index-extractors-entity) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<0.24.0->llama-index-extractors-entity) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<0.24.0->llama-index-extractors-entity) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<0.24.0->llama-index-extractors-entity) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<0.24.0->llama-index-extractors-entity) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<0.24.0->llama-index-extractors-entity) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub<0.24.0->llama-index-extractors-entity) (4.11.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (3.10.8)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.2.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (10.4.0)\n",
      "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (0.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.16.0)\n",
      "Requirement already satisfied: torch in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker>=1.5.0->llama-index-extractors-entity) (2.5.1+cu118)\n",
      "Requirement already satisfied: accelerate in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker>=1.5.0->llama-index-extractors-entity) (1.1.1)\n",
      "Requirement already satisfied: transformers>=4.19.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker>=1.5.0->llama-index-extractors-entity) (4.46.2)\n",
      "Requirement already satisfied: datasets>=2.14.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker>=1.5.0->llama-index-extractors-entity) (3.1.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker>=1.5.0->llama-index-extractors-entity) (0.4.3)\n",
      "Requirement already satisfied: seqeval in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker>=1.5.0->llama-index-extractors-entity) (1.2.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker>=1.5.0->llama-index-extractors-entity) (3.1.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.17.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker>=1.5.0->llama-index-extractors-entity) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker>=1.5.0->llama-index-extractors-entity) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker>=1.5.0->llama-index-extractors-entity) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker>=1.5.0->llama-index-extractors-entity) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker>=1.5.0->llama-index-extractors-entity) (0.70.16)\n",
      "Requirement already satisfied: click in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests->huggingface-hub<0.24.0->llama-index-extractors-entity) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests->huggingface-hub<0.24.0->llama-index-extractors-entity) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests->huggingface-hub<0.24.0->llama-index-extractors-entity) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests->huggingface-hub<0.24.0->llama-index-extractors-entity) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<0.24.0->llama-index-extractors-entity) (0.4.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from transformers>=4.19.0->span-marker>=1.5.0->llama-index-extractors-entity) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from transformers>=4.19.0->span-marker>=1.5.0->llama-index-extractors-entity) (0.20.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.0.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from accelerate->span-marker>=1.5.0->llama-index-extractors-entity) (5.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from torch->span-marker>=1.5.0->llama-index-extractors-entity) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from sympy==1.13.1->torch->span-marker>=1.5.0->llama-index-extractors-entity) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (3.23.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from jinja2->span-marker>=1.5.0->llama-index-extractors-entity) (3.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from seqeval->span-marker>=1.5.0->llama-index-extractors-entity) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->span-marker>=1.5.0->llama-index-extractors-entity) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->span-marker>=1.5.0->llama-index-extractors-entity) (3.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-extractors-entity) (0.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pandas->datasets>=2.14.0->span-marker>=1.5.0->llama-index-extractors-entity) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pandas->datasets>=2.14.0->span-marker>=1.5.0->llama-index-extractors-entity) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pandas->datasets>=2.14.0->span-marker>=1.5.0->llama-index-extractors-entity) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.14.0->span-marker>=1.5.0->llama-index-extractors-entity) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-extractors-entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: span-marker in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: torch in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker) (2.5.1+cu118)\n",
      "Requirement already satisfied: accelerate in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker) (1.1.1)\n",
      "Requirement already satisfied: transformers>=4.19.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker) (4.46.2)\n",
      "Requirement already satisfied: datasets>=2.14.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker) (3.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker) (23.2)\n",
      "Requirement already satisfied: evaluate in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker) (0.4.3)\n",
      "Requirement already satisfied: seqeval in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker) (1.2.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker) (3.1.4)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from span-marker) (0.23.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker) (4.67.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.14.0->span-marker) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker) (3.10.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from datasets>=2.14.0->span-marker) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from huggingface-hub->span-marker) (4.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from transformers>=4.19.0->span-marker) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from transformers>=4.19.0->span-marker) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from transformers>=4.19.0->span-marker) (0.20.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from accelerate->span-marker) (5.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from torch->span-marker) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from torch->span-marker) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from sympy==1.13.1->torch->span-marker) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from jinja2->span-marker) (3.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from seqeval->span-marker) (1.5.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp->datasets>=2.14.0->span-marker) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp->datasets>=2.14.0->span-marker) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp->datasets>=2.14.0->span-marker) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp->datasets>=2.14.0->span-marker) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp->datasets>=2.14.0->span-marker) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp->datasets>=2.14.0->span-marker) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests>=2.32.2->datasets>=2.14.0->span-marker) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests>=2.32.2->datasets>=2.14.0->span-marker) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests>=2.32.2->datasets>=2.14.0->span-marker) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests>=2.32.2->datasets>=2.14.0->span-marker) (2024.8.30)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->span-marker) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->span-marker) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval->span-marker) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from tqdm>=4.66.3->datasets>=2.14.0->span-marker) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pandas->datasets>=2.14.0->span-marker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pandas->datasets>=2.14.0->span-marker) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pandas->datasets>=2.14.0->span-marker) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.14.0->span-marker) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.14.0->span-marker) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install span-marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "from span_marker import SpanMarkerModel\n",
    "\n",
    "model = SpanMarkerModel.from_pretrained(\"tomaarsen/span-marker-mbert-base-multinerd\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in EntityExtractor has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "entity_extractor = EntityExtractor(\n",
    "    prediction_threshold=0.5,\n",
    "    label_entities=False,  # include the entity label in the metadata (can be erroneous)\n",
    "    device=\"cuda\",  # set to \"cuda\" if you have a GPU or \"cpu\" if you have CPU alone\n",
    "    model_name=\"tomaarsen/span-marker-mbert-base-multinerd\",  # Local model name\n",
    "    entity_types=[\"PERSON\", \"ORG\", \"GPE\"],\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "test_document_1 = Document(text=\"\"\"\n",
    "                        LLMs offer a natural language interface between humans and data. LLMs come pre-trained on huge amounts of publicly available data, but they are not trained on your data. Your data may be private or specific to the problem you're trying to solve. It's behind APIs, in SQL databases, or trapped in PDFs and slide decks.\n",
    "                        Context augmentation makes your data available to the LLM to solve the problem at hand. LlamaIndex provides the tools to build any of context-augmentation use case, from prototype to production. Our tools allow you to ingest, parse, index and process your data and quickly implement complex query workflows combining data access with LLM prompting.\n",
    "                        The most popular example of context-augmentation is Retrieval-Augmented Generation or RAG, which combines context with LLMs at inference time.\"\"\")\n",
    "test_document_2 = Document(text=\"\"\" \n",
    "                        Agents are LLM-powered knowledge assistants that use tools to perform tasks like research, data extraction, and more. Agents range from simple question-answering to being able to sense, decide and take actions in order to complete tasks.\n",
    "                        LlamaIndex provides a framework for building agents including the ability to use RAG pipelines as one of many tools to complete a task.\"\"\")\n",
    "\n",
    "test_documents = [test_document_1,test_document_2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Redownload the punkt resource\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting entities: 100%|██████████| 2/2 [00:00<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "entity_result = await entity_extractor.aprocess_nodes(test_documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='f1460642-b13a-43bd-8791-29e0fb379f5d', embedding=None, metadata={'entities': ['LlamaIndex', 'SQL']}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"\\n                        LLMs offer a natural language interface between humans and data. LLMs come pre-trained on huge amounts of publicly available data, but they are not trained on your data. Your data may be private or specific to the problem you're trying to solve. It's behind APIs, in SQL databases, or trapped in PDFs and slide decks.\\n                        Context augmentation makes your data available to the LLM to solve the problem at hand. LlamaIndex provides the tools to build any of context-augmentation use case, from prototype to production. Our tools allow you to ingest, parse, index and process your data and quickly implement complex query workflows combining data access with LLM prompting.\\n                        The most popular example of context-augmentation is Retrieval-Augmented Generation or RAG, which combines context with LLMs at inference time.\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
       " Document(id_='60631b38-f9ff-4587-b066-6c38a8ea4e72', embedding=None, metadata={'entities': ['LlamaIndex']}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=' \\n                        Agents are LLM-powered knowledge assistants that use tools to perform tasks like research, data extraction, and more. Agents range from simple question-answering to being able to sense, decide and take actions in order to complete tasks.\\n                        LlamaIndex provides a framework for building agents including the ability to use RAG pipelines as one of many tools to complete a task.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entity Output:\n",
      "Node ID: f1460642-b13a-43bd-8791-29e0fb379f5d, Entities: ['LlamaIndex', 'SQL']\n",
      "Node ID: 60631b38-f9ff-4587-b066-6c38a8ea4e72, Entities: ['LlamaIndex']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nEntity Output:\")\n",
    "for node in entity_result:\n",
    "    print(f\"Node ID: {node.node_id}, Entities: {node.metadata.get('entities', 'No entities available')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter()\n",
    "\n",
    "transformations = [node_parser, entity_extractor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting entities: 100%|██████████| 2/2 [00:00<00:00,  5.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "\n",
    "nodes = pipeline.run(documents=test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='b150cb62-900e-4cb8-9a5c-0a6a82e5f3e7', embedding=None, metadata={'entities': ['SQL']}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f1460642-b13a-43bd-8791-29e0fb379f5d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'entities': ['LlamaIndex', 'SQL']}, hash='cc6b43aa8335c6efe1e351005bb7ee363a12e227222b42d4682c185062d7ae45')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"LLMs offer a natural language interface between humans and data. LLMs come pre-trained on huge amounts of publicly available data, but they are not trained on your data. Your data may be private or specific to the problem you're trying to solve. It's behind APIs, in SQL databases, or trapped in PDFs and slide decks.\\n                        Context augmentation makes your data available to the LLM to solve the problem at hand. LlamaIndex provides the tools to build any of context-augmentation use case, from prototype to production. Our tools allow you to ingest, parse, index and process your data and quickly implement complex query workflows combining data access with LLM prompting.\\n                        The most popular example of context-augmentation is Retrieval-Augmented Generation or RAG, which combines context with LLMs at inference time.\", mimetype='text/plain', start_char_idx=25, end_char_idx=882, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
       " TextNode(id_='a109fc78-9365-4447-b9cf-170fbb358e27', embedding=None, metadata={'entities': ['LlamaIndex']}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='60631b38-f9ff-4587-b066-6c38a8ea4e72', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'entities': ['LlamaIndex']}, hash='c6502754b0be7c06fd423f08e3df84fffc354d44baea27253f2575728043f98e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Agents are LLM-powered knowledge assistants that use tools to perform tasks like research, data extraction, and more. Agents range from simple question-answering to being able to sense, decide and take actions in order to complete tasks.\\n                        LlamaIndex provides a framework for building agents including the ability to use RAG pipelines as one of many tools to complete a task.', mimetype='text/plain', start_char_idx=26, end_char_idx=423, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: b150cb62-900e-4cb8-9a5c-0a6a82e5f3e7, Entities: ['SQL']\n",
      "======================================================\n",
      "Node ID: a109fc78-9365-4447-b9cf-170fbb358e27, Entities: ['LlamaIndex']\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "for node in nodes:\n",
    "    print(f\"Node ID: {node.node_id}, Entities: {node.metadata.get('entities', 'No entities available')}\")\n",
    "    print(\"======================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "#documents = SimpleDirectoryReader(input_files=['../data/2022 Q3 AAPL.pdf']).load_data()\n",
    "documents = SimpleDirectoryReader(input_files=['../data/paul_graham_essay3.txt']).load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=1024, chunk_overlap=50),  # Split text into manageable chunks\n",
    "        entity_extractor,  # Extract summaries\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 25.00it/s]\n",
      "Extracting entities: 100%|██████████| 18/18 [00:03<00:00,  5.58it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes = pipeline.run(\n",
    "    documents=documents,\n",
    "    in_place=True,\n",
    "    show_progress=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 17cc4217-2460-4b4c-87ad-8a814fe5b97d, Entities: ['Rich Draves', 'IBM']\n",
      "======================================================\n",
      "Node ID: 98310f07-0c47-44cf-a60b-7dd60515223a, Entities: ['Winograd', 'Cornell']\n",
      "======================================================\n",
      "Node ID: 06476864-c896-44fd-9544-d9013c93d3e7, Entities: ['Rich Draves', 'Xerox Dandelions', 'Carnegie Institute', 'CMU']\n",
      "======================================================\n",
      "Node ID: cc5cc40e-acae-45fd-b8a2-905e1559037d, Entities: ['Cezanne', 'Florence']\n",
      "======================================================\n",
      "Node ID: 96e41351-5d7c-4df7-81bf-7ca509adb524, Entities: ['Ulivi', 'Interleaf', 'Microsoft Word', 'Italian']\n",
      "======================================================\n",
      "Node ID: e3d5a610-e770-4017-b08d-9579f92cba02, Entities: ['Roy Lichtenstein']\n",
      "======================================================\n",
      "Node ID: 7ca997dd-830f-4b41-bb0a-7d19274a520b, Entities: ['Y Combinator', 'Robert']\n",
      "======================================================\n",
      "Node ID: 4b53d6f2-2299-4450-aaf5-820fa9e99da8, Entities: ['Robert', 'Trevor', 'Trevor Blackwell']\n",
      "======================================================\n",
      "Node ID: 739470d7-5c35-4b5d-931f-9fe177909f32, Entities: ['Y Combinator']\n",
      "======================================================\n",
      "Node ID: a20ba74b-663e-4e04-a654-41e2e1767755, Entities: ['California', 'New York', 'Santa Cruz Mountains']\n",
      "======================================================\n",
      "Node ID: 63816175-e7a3-43a4-8f44-f9e3157302d1, Entities: ['Y Combinator', 'Lisp']\n",
      "======================================================\n",
      "Node ID: e302bb39-c07f-42dd-8569-e95659fff952, Entities: ['Maria Daniels', 'Cambridge', 'Hackers & Painters']\n",
      "======================================================\n",
      "Node ID: aa581700-fd7f-44c6-8e5f-952bc44ca1b5, Entities: ['Julian']\n",
      "======================================================\n",
      "Node ID: 4705f4b1-7a94-499a-8655-ba72260c65e0, Entities: ['Robert', 'YC']\n",
      "======================================================\n",
      "Node ID: aec38903-c7d2-4caf-a2a5-08731d81a39a, Entities: ['colon cancer']\n",
      "======================================================\n",
      "Node ID: 063575d8-ba49-459d-874b-ed07e378dc30, Entities: ['Steve Russell', 'IBM 704', 'Russell', 'John McCarthy']\n",
      "======================================================\n",
      "Node ID: 2ffb1fae-166d-4cc6-b4ba-256527baa3e2, Entities: ['Italian']\n",
      "======================================================\n",
      "Node ID: 17727e73-ae8a-4040-b1fd-21191fa6c880, Entities: ['Silicon Valley', 'Y Combinator', 'Cambridge Seed']\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "for node in nodes:\n",
    "    print(f\"Node ID: {node.node_id}, Entities: {node.metadata.get('entities', 'No entities available')}\")\n",
    "    print(\"======================================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
