{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=['../data/paul_graham_essay3.txt']).load_data()\n",
    "# documents = SimpleDirectoryReader(input_files=['../data/2022 Q3 AAPL.pdf']).load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create an index from the documents\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "\n",
    "# Query engine for handling queries\n",
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from typing import Dict, List, Any\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class QueryMultiStepEvent(Event):\n",
    "    \"\"\"\n",
    "    Event containing results of a multi-step query process.\n",
    "\n",
    "    Attributes:\n",
    "        nodes (List[NodeWithScore]): Nodes with scores.\n",
    "        source_nodes (List[NodeWithScore]): Source nodes.\n",
    "        final_response_metadata (Dict[str, Any]): Metadata for the response.\n",
    "    \"\"\"\n",
    "    nodes: List[NodeWithScore]\n",
    "    source_nodes: List[NodeWithScore]\n",
    "    final_response_metadata: Dict[str, Any]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow, Context, StartEvent, StopEvent, step\n",
    ")\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "from llama_index.core.schema import QueryBundle, TextNode\n",
    "from typing import cast, Dict, Any\n",
    "\n",
    "\n",
    "class MultiStepQueryEngineWorkflow(Workflow):\n",
    "\n",
    "    def combine_queries(\n",
    "        self, query_bundle: QueryBundle, prev_reasoning: str, index_summary: str, llm\n",
    "    ) -> QueryBundle:\n",
    "        \"\"\"\n",
    "        Dynamically refine the query for the next step using LLM.\n",
    "\n",
    "        Parameters:\n",
    "        - query_bundle: Current query.\n",
    "        - prev_reasoning: All prior questions and answers.\n",
    "        - index_summary: High-level summary of the index data.\n",
    "        \"\"\"\n",
    "        prompt_template = (\n",
    "            \"You are solving a multi-step question step by step.\\n\"\n",
    "            \"Index Summary: {index_summary}\\n\\n\"\n",
    "            \"Previous Steps and Answers:\\n{prev_reasoning}\\n\\n\"\n",
    "            \"Current Query: {current_query}\\n\\n\"\n",
    "            \"Based on the above context, generate the next refined sub-question. \"\n",
    "            \"If no further refinement is needed, respond with 'None'.\\n\\n\"\n",
    "            \"Refined Query:\"\n",
    "        )\n",
    "\n",
    "        # Build prompt with current context\n",
    "        detailed_prompt = prompt_template.format(\n",
    "            index_summary=index_summary,\n",
    "            prev_reasoning=prev_reasoning.strip(),\n",
    "            current_query=query_bundle.query_str,\n",
    "        )\n",
    "\n",
    "        # Generate the next refined query using the LLM\n",
    "        refined_query = llm.complete(detailed_prompt).text.strip()\n",
    "\n",
    "        # Handle stopping condition\n",
    "        if refined_query.lower() in [\"none\", \"no query\", \"stop\"]:\n",
    "            refined_query = \"None\"\n",
    "\n",
    "        return QueryBundle(query_str=refined_query)\n",
    "\n",
    "\n",
    "\n",
    "    def default_stop_fn(self, stop_dict: Dict) -> bool:\n",
    "        query_bundle = cast(QueryBundle, stop_dict.get(\"query_bundle\"))\n",
    "        return \"none\" in query_bundle.query_str.lower()\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def query_multistep(self, ctx: Context, ev: StartEvent) -> QueryMultiStepEvent:\n",
    "        \"\"\"Execute the multi-step query process.\"\"\"\n",
    "        prev_reasoning, cur_steps = \"\", 0\n",
    "        should_stop = False\n",
    "\n",
    "        # Extract required inputs\n",
    "        num_steps = ev.get(\"num_steps\", 3)\n",
    "        query = ev.get(\"query\")\n",
    "        if not query:\n",
    "            raise ValueError(\"Query must be provided in StartEvent.\")\n",
    "        \n",
    "        index_summary = ev.get(\"index_summary\", \"\")\n",
    "        llm, query_engine = Settings.llm, ev.get(\"query_engine\")\n",
    "\n",
    "        # Initialize metadata and results\n",
    "        text_chunks, source_nodes = [], []\n",
    "        final_response_metadata: Dict[str, Any] = {\"sub_qa\": []}\n",
    "\n",
    "        # Set initial query into the context\n",
    "        await ctx.set(\"query\", query)\n",
    "\n",
    "        while not should_stop:\n",
    "            if cur_steps >= num_steps:\n",
    "                break\n",
    "\n",
    "            # Retrieve the query dynamically from context\n",
    "            query = await ctx.get(\"query\")\n",
    "\n",
    "            # Generate refined query\n",
    "            updated_query_bundle = self.combine_queries(\n",
    "                QueryBundle(query_str=query), prev_reasoning, index_summary, llm\n",
    "            )\n",
    "\n",
    "            # Check for stop condition\n",
    "            if \"none\" in updated_query_bundle.query_str.lower():\n",
    "                should_stop = True\n",
    "                break\n",
    "\n",
    "            print(f\"Step {cur_steps}: {updated_query_bundle.query_str}\")\n",
    "\n",
    "            # Execute the query and append results\n",
    "            cur_response = query_engine.query(updated_query_bundle)\n",
    "\n",
    "            # Update context and reasoning\n",
    "            await ctx.set(\"query\", updated_query_bundle.query_str)  # Set refined query back in context\n",
    "            cur_qa_text = f\"Q: {updated_query_bundle.query_str}\\nA: {cur_response}\"\n",
    "            text_chunks.append(cur_qa_text)\n",
    "            source_nodes.extend(cur_response.source_nodes)\n",
    "\n",
    "            # Update metadata\n",
    "            final_response_metadata[\"sub_qa\"].append((updated_query_bundle.query_str, cur_response))\n",
    "            prev_reasoning += f\"- Question: {updated_query_bundle.query_str}\\n- Answer: {cur_response}\\n\"\n",
    "            cur_steps += 1\n",
    "\n",
    "        # Prepare response event\n",
    "        nodes = [NodeWithScore(node=TextNode(text=text_chunk)) for text_chunk in text_chunks]\n",
    "        return QueryMultiStepEvent(nodes=nodes, source_nodes=source_nodes, final_response_metadata=final_response_metadata)\n",
    "\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def synthesize(self, ctx: Context, ev: QueryMultiStepEvent) -> StopEvent:\n",
    "        response_synthesizer = get_response_synthesizer()\n",
    "        query = await ctx.get(\"query\")\n",
    "        final_response = await response_synthesizer.asynthesize(\n",
    "            query=query, nodes=ev.nodes, additional_source_nodes=ev.source_nodes\n",
    "        )\n",
    "        final_response.metadata = ev.final_response_metadata\n",
    "        return StopEvent(result=final_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the workflow\n",
    "w = MultiStepQueryEngineWorkflow(timeout=200)\n",
    "\n",
    "async def main():\n",
    "    result = await w.run(\n",
    "        query=\"In which city did the author found his first company, Viaweb?\",\n",
    "        query_engine=query_engine,\n",
    "        index_summary=\"Used to answer questions about the author and Viaweb.\",\n",
    "        num_steps=3,\n",
    "    )\n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(result)\n",
    "    print(\"\\nStep-by-Step Reasoning:\")\n",
    "    sub_qa = result.metadata[\"sub_qa\"]\n",
    "    for idx, (question, answer) in enumerate(sub_qa):\n",
    "        print(f\"Step {idx}:\")\n",
    "        print(f\"  Question: {question}\")\n",
    "        print(f\"  Answer: {answer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w, filename=\"workflow_capstone.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
