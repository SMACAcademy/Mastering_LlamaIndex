{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (0.12.5)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.12.5)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.6.3)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.9.48.post4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.3.10)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.57.4)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.5->llama-index) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (3.10.8)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (2024.9.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (3.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.5->llama-index) (1.16.0)\n",
      "Requirement already satisfied: llama-cloud>=0.1.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.1.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.5.13)\n",
      "Requirement already satisfied: click in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.5->llama-index) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.5->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.5->llama-index) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.5->llama-index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.5->llama-index) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.5->llama-index) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.5->llama-index) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.5->llama-index) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.5->llama-index) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.5->llama-index) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.5->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.5->llama-index) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from click->nltk>3.8.1->llama-index) (0.4.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.5->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.5->llama-index) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.5->llama-index) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.5->llama-index) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.5->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.5->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.5->llama-index) (3.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.5->llama-index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.5->llama-index) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U llama-index\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Sample data download loacation : https://github.com/user-attachments/files/16474262/data.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Set, FrozenSet\n",
    "\n",
    "from llama_index.core.schema import BaseNode, TextNode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# constants\n",
    "DEFAULT_CHUNK_SIZE = 2048  # optionally splits documents into CHUNK_SIZE, then regroups them to demonstrate grouping algorithm\n",
    "DEFAULT_MAX_GROUP_SIZE = 20  # maximum number of documents in a group\n",
    "DEFAULT_SMALL_CHUNK_SIZE = 512  # small chunk size for generating embeddings\n",
    "DEFAULT_TOP_K = 8  # top k for retrieving\n",
    "\n",
    "\n",
    "def split_doc(chunk_size: int, documents: List[BaseNode]) -> List[TextNode]:\n",
    "    \"\"\"Splits documents into smaller pieces.\n",
    "\n",
    "    Args:\n",
    "        chunk_size (int): Chunk size\n",
    "        documents (List[BaseNode]): Documents\n",
    "\n",
    "    Returns:\n",
    "        List[TextNode]: Smaller chunks\n",
    "    \"\"\"\n",
    "    # split docs into tokens\n",
    "    text_parser = SentenceSplitter(chunk_size=chunk_size)\n",
    "    return text_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "\n",
    "def group_docs(\n",
    "    nodes: List[str],\n",
    "    adj: Dict[str, List[str]],\n",
    "    max_group_size: Optional[int] = DEFAULT_MAX_GROUP_SIZE,\n",
    ") -> Set[FrozenSet[str]]:\n",
    "    \"\"\"Groups documents.\n",
    "\n",
    "    Args:\n",
    "        nodes (List[str]): documents IDs\n",
    "        adj (Dict[str, List[str]]): related documents for each document; id -> list of doc strings\n",
    "        max_group_size (Optional[int], optional): max group size, None if no max group size. Defaults to DEFAULT_MAX_GROUP_SIZE.\n",
    "    \"\"\"\n",
    "    docs = sorted(nodes, key=lambda node: len(adj[node]))\n",
    "    groups = set()  # set of set of IDs\n",
    "    for d in docs:\n",
    "        related_groups = set()\n",
    "        for r in adj[d]:\n",
    "            for g in groups:\n",
    "                if r in g:\n",
    "                    related_groups = related_groups.union(frozenset([g]))\n",
    "\n",
    "        gnew = {d}\n",
    "        related_groupsl = sorted(related_groups, key=lambda el: len(el))\n",
    "        for g in related_groupsl:\n",
    "            if max_group_size is None or len(gnew) + len(g) <= max_group_size:\n",
    "                gnew = gnew.union(g)\n",
    "                if g in groups:\n",
    "                    groups.remove(g)\n",
    "\n",
    "        groups.add(frozenset(gnew))\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def get_grouped_docs(\n",
    "    nodes: List[TextNode],\n",
    "    max_group_size: Optional[int] = DEFAULT_MAX_GROUP_SIZE,\n",
    ") -> List[TextNode]:\n",
    "    \"\"\"Gets list of documents that are grouped.\n",
    "\n",
    "    Args:\n",
    "        nodes (t.List[TextNode]): Input list\n",
    "        max_group_size (Optional[int], optional): max group size, None if no max group size. Defaults to DEFAULT_MAX_GROUP_SIZE.\n",
    "\n",
    "    Returns:\n",
    "        t.List[TextNode]: Output list\n",
    "    \"\"\"\n",
    "    # node IDs\n",
    "    nodes_str = [node.id_ for node in nodes]\n",
    "    # maps node ID -> related node IDs based on that node's relationships\n",
    "    adj: Dict[str, List[str]] = {\n",
    "        node.id_: [val.node_id for val in node.relationships.values()]\n",
    "        for node in nodes\n",
    "    }\n",
    "    # node ID -> node\n",
    "    nodes_dict = {node.id_: node for node in nodes}\n",
    "\n",
    "    res = group_docs(nodes_str, adj, max_group_size)\n",
    "\n",
    "    ret_nodes = []\n",
    "    for g in res:\n",
    "        cur_node = TextNode()\n",
    "\n",
    "        for node_id in g:\n",
    "            cur_node.text += nodes_dict[node_id].text + \"\\n\\n\"\n",
    "            cur_node.metadata.update(nodes_dict[node_id].metadata)\n",
    "\n",
    "        ret_nodes.append(cur_node)\n",
    "\n",
    "    return ret_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.vector_stores.simple import BasePydanticVectorStore\n",
    "from llama_index.core.schema import QueryBundle, NodeWithScore\n",
    "from llama_index.core.vector_stores.types import VectorStoreQuery\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "\n",
    "class LongRAGRetriever(BaseRetriever):\n",
    "    \"\"\"Long RAG Retriever.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        grouped_nodes: List[TextNode],\n",
    "        small_toks: List[TextNode],\n",
    "        vector_store: BasePydanticVectorStore,\n",
    "        similarity_top_k: int = DEFAULT_TOP_K,\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "        Args:\n",
    "            grouped_nodes (List[TextNode]): Long retrieval units, nodes with docs grouped together based on relationships\n",
    "            small_toks (List[TextNode]): Smaller tokens\n",
    "            embed_model (BaseEmbedding, optional): Embed model. Defaults to None.\n",
    "            similarity_top_k (int, optional): Similarity top k. Defaults to 8.\n",
    "        \"\"\"\n",
    "        self._grouped_nodes = grouped_nodes\n",
    "        self._grouped_nodes_dict = {node.id_: node for node in grouped_nodes}\n",
    "        self._small_toks = small_toks\n",
    "        self._small_toks_dict = {node.id_: node for node in self._small_toks}\n",
    "\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._vec_store = vector_store\n",
    "        self._embed_model = Settings.embed_model\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieves.\n",
    "\n",
    "        Args:\n",
    "            query_bundle (QueryBundle): query bundle\n",
    "\n",
    "        Returns:\n",
    "            List[NodeWithScore]: nodes with scores\n",
    "        \"\"\"\n",
    "        # make query\n",
    "        query_embedding = self._embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding, similarity_top_k=500\n",
    "        )\n",
    "\n",
    "        # query for answer\n",
    "        query_res = self._vec_store.query(vector_store_query)\n",
    "\n",
    "        # determine top parents of most similar children (these are long retrieval units)\n",
    "        top_parents_set: Set[str] = set()\n",
    "        top_parents: List[NodeWithScore] = []\n",
    "        for id_, similarity in zip(query_res.ids, query_res.similarities):\n",
    "            cur_node = self._small_toks_dict[id_]\n",
    "            parent_id = cur_node.ref_doc_id\n",
    "            if parent_id not in top_parents_set:\n",
    "                top_parents_set.add(parent_id)\n",
    "\n",
    "                parent_node = self._grouped_nodes_dict[parent_id]\n",
    "                node_with_score = NodeWithScore(\n",
    "                    node=parent_node, score=similarity\n",
    "                )\n",
    "                top_parents.append(node_with_score)\n",
    "\n",
    "                if len(top_parents_set) >= self._similarity_top_k:\n",
    "                    break\n",
    "\n",
    "        assert len(top_parents) == min(\n",
    "            self._similarity_top_k, len(self._grouped_nodes)\n",
    "        )\n",
    "\n",
    "        return top_parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.llms import LLM\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "\n",
    "class LoadNodeEvent(Event):\n",
    "    \"\"\"Event for loading nodes.\"\"\"\n",
    "\n",
    "    small_nodes: Iterable[TextNode]\n",
    "    grouped_nodes: list[TextNode]\n",
    "    index: VectorStoreIndex\n",
    "    similarity_top_k: int\n",
    "    llm: LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    step,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Context,\n",
    ")\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "class LongRAGWorkflow(Workflow):\n",
    "    \"\"\"Long RAG Workflow.\"\"\"\n",
    "\n",
    "    @step\n",
    "    async def ingest(self, ev: StartEvent) -> LoadNodeEvent | None:\n",
    "        \"\"\"Ingestion step.\n",
    "\n",
    "        Args:\n",
    "            ctx (Context): Context\n",
    "            ev (StartEvent): start event\n",
    "\n",
    "        Returns:\n",
    "            StopEvent | None: stop event with result\n",
    "        \"\"\"\n",
    "        data_dir: str = ev.get(\"data_dir\")\n",
    "        llm: LLM = ev.get(\"llm\")\n",
    "        chunk_size: int | None = ev.get(\"chunk_size\")\n",
    "        similarity_top_k: int = ev.get(\"similarity_top_k\")\n",
    "        small_chunk_size: int = ev.get(\"small_chunk_size\")\n",
    "        index: VectorStoreIndex | None = ev.get(\"index\")\n",
    "        index_kwargs: dict[str, t.Any] | None = ev.get(\"index_kwargs\")\n",
    "\n",
    "        if any(\n",
    "            i is None\n",
    "            for i in [data_dir, llm, similarity_top_k, small_chunk_size]\n",
    "        ):\n",
    "            return None\n",
    "\n",
    "        if not index:\n",
    "            docs = SimpleDirectoryReader(data_dir).load_data()\n",
    "            if chunk_size is not None:\n",
    "                nodes = split_doc(\n",
    "                    chunk_size, docs\n",
    "                )  # split documents into chunks of chunk_size\n",
    "                grouped_nodes = get_grouped_docs(\n",
    "                    nodes\n",
    "                )  # get list of nodes after grouping (groups are combined into one node), these are long retrieval units\n",
    "            else:\n",
    "                grouped_nodes = docs\n",
    "\n",
    "            # split large retrieval units into smaller nodes\n",
    "            small_nodes = split_doc(small_chunk_size, grouped_nodes)\n",
    "\n",
    "            index_kwargs = index_kwargs or {}\n",
    "            index = VectorStoreIndex(small_nodes, **index_kwargs)\n",
    "        else:\n",
    "            # get smaller nodes from index and form large retrieval units from these nodes\n",
    "            small_nodes = index.docstore.docs.values()\n",
    "            grouped_nodes = get_grouped_docs(small_nodes, None)\n",
    "\n",
    "        return LoadNodeEvent(\n",
    "            small_nodes=small_nodes,\n",
    "            grouped_nodes=grouped_nodes,\n",
    "            index=index,\n",
    "            similarity_top_k=similarity_top_k,\n",
    "            llm=llm,\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def make_query_engine(\n",
    "        self, ctx: Context, ev: LoadNodeEvent\n",
    "    ) -> StopEvent:\n",
    "        \"\"\"Query engine construction step.\n",
    "\n",
    "        Args:\n",
    "            ctx (Context): context\n",
    "            ev (LoadNodeEvent): event\n",
    "\n",
    "        Returns:\n",
    "            StopEvent: stop event\n",
    "        \"\"\"\n",
    "        # make retriever and query engine\n",
    "        retriever = LongRAGRetriever(\n",
    "            grouped_nodes=ev.grouped_nodes,\n",
    "            small_toks=ev.small_nodes,\n",
    "            similarity_top_k=ev.similarity_top_k,\n",
    "            vector_store=ev.index.vector_store,\n",
    "        )\n",
    "        query_eng = RetrieverQueryEngine.from_args(retriever, ev.llm)\n",
    "\n",
    "        return StopEvent(\n",
    "            result={\n",
    "                \"retriever\": retriever,\n",
    "                \"query_engine\": query_eng,\n",
    "                \"index\": ev.index,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def query(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        \"\"\"Query step.\n",
    "\n",
    "        Args:\n",
    "            ctx (Context): context\n",
    "            ev (StartEvent): start event\n",
    "\n",
    "        Returns:\n",
    "            StopEvent | None: stop event with result\n",
    "        \"\"\"\n",
    "        query_str: str | None = ev.get(\"query_str\")\n",
    "        query_eng = ev.get(\"query_eng\")\n",
    "\n",
    "        if query_str is None:\n",
    "            return None\n",
    "\n",
    "        result = query_eng.query(query_str)\n",
    "        return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = LongRAGWorkflow(timeout=60)\n",
    "#llm = OpenAI(\"gpt-4o\")\n",
    "llm = ollama_llm\n",
    "data_dir = \"workflow_data1\"\n",
    "\n",
    "# initialize the workflow\n",
    "result = await wf.run(\n",
    "    data_dir=data_dir,\n",
    "    llm=llm,\n",
    "    chunk_size=DEFAULT_CHUNK_SIZE,\n",
    "    similarity_top_k=DEFAULT_TOP_K,\n",
    "    small_chunk_size=DEFAULT_SMALL_CHUNK_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'query': error reading llm response: read tcp 127.0.0.1:51194->127.0.0.1:50061: wsarecv: An existing connection was forcibly closed by the remote host.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:247\u001b[0m, in \u001b[0;36mWorkflow._start.<locals>._task\u001b[1;34m(name, queue, step, config)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     new_ev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# exit the retrying loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:367\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 367\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[40], line 117\u001b[0m, in \u001b[0;36mLongRAGWorkflow.query\u001b[1;34m(self, ctx, ev)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m result \u001b[38;5;241m=\u001b[39m query_eng\u001b[38;5;241m.\u001b[39mquery(query_str)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StopEvent(result\u001b[38;5;241m=\u001b[39mresult)\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     51\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 52\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query(str_or_query_bundle)\n\u001b[0;32m     53\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m     54\u001b[0m     QueryEndEvent(query\u001b[38;5;241m=\u001b[39mstr_or_query_bundle, response\u001b[38;5;241m=\u001b[39mquery_result)\n\u001b[0;32m     55\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:179\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    178\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m--> 179\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[0;32m    180\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[0;32m    181\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    182\u001b[0m )\n\u001b[0;32m    183\u001b[0m query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\base.py:241\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[1;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    238\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE,\n\u001b[0;32m    239\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[0;32m    240\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m--> 241\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m    242\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[0;32m    243\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    244\u001b[0m             n\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[0;32m    245\u001b[0m         ],\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m    247\u001b[0m     )\n\u001b[0;32m    249\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\compact_and_refine.py:43\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m new_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m     44\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[0;32m     45\u001b[0m     text_chunks\u001b[38;5;241m=\u001b[39mnew_texts,\n\u001b[0;32m     46\u001b[0m     prev_response\u001b[38;5;241m=\u001b[39mprev_response,\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m     48\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:184\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refine_response_single(\n\u001b[0;32m    185\u001b[0m         prev_response, query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[0;32m    186\u001b[0m     )\n\u001b[0;32m    187\u001b[0m prev_response \u001b[38;5;241m=\u001b[39m response\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:321\u001b[0m, in \u001b[0;36mRefine._refine_response_single\u001b[1;34m(self, response, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     structured_response \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m    320\u001b[0m         StructuredRefineResponse,\n\u001b[1;32m--> 321\u001b[0m         program(\n\u001b[0;32m    322\u001b[0m             context_msg\u001b[38;5;241m=\u001b[39mcur_text_chunk,\n\u001b[0;32m    323\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m    324\u001b[0m         ),\n\u001b[0;32m    325\u001b[0m     )\n\u001b[0;32m    326\u001b[0m     query_satisfied \u001b[38;5;241m=\u001b[39m structured_response\u001b[38;5;241m.\u001b[39mquery_satisfied\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:85\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt,\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m     88\u001b[0m     )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[38;5;241m=\u001b[39manswer, query_satisfied\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py:596\u001b[0m, in \u001b[0;36mLLM.predict\u001b[1;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[0;32m    595\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_messages(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[1;32m--> 596\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat(messages)\n\u001b[0;32m    597\u001b[0m output \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:173\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[1;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m f(_self, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\llms\\ollama\\base.py:286\u001b[0m, in \u001b[0;36mOllama.chat\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m tools \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 286\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat(\n\u001b[0;32m    287\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    288\u001b[0m     messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[0;32m    289\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    291\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    292\u001b[0m     options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_kwargs,\n\u001b[0;32m    293\u001b[0m     keep_alive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive,\n\u001b[0;32m    294\u001b[0m )\n\u001b[0;32m    296\u001b[0m tool_calls \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\ollama\\_client.py:236\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    234\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [_encode_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_stream(\n\u001b[0;32m    237\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    238\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    239\u001b[0m   json\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model,\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m: messages,\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m'\u001b[39m: tools \u001b[38;5;129;01mor\u001b[39;00m [],\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m: stream,\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m'\u001b[39m: options \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m'\u001b[39m: keep_alive,\n\u001b[0;32m    247\u001b[0m   },\n\u001b[0;32m    248\u001b[0m   stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    249\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\ollama\\_client.py:99\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[1;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m     96\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[1;32m---> 99\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\ollama\\_client.py:75\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 75\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mResponseError\u001b[0m: error reading llm response: read tcp 127.0.0.1:51194->127.0.0.1:50061: wsarecv: An existing connection was forcibly closed by the remote host.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mWorkflowRuntimeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Markdown\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# run a query\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m wf\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m      5\u001b[0m     query_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow can Pittsburgh become a startup hub, and what are the two types of moderates?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     query_eng\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_engine\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m display(Markdown(\u001b[38;5;28mstr\u001b[39m(res)))\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\asyncio\\futures.py:287\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\asyncio\\tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 349\u001b[0m         future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:438\u001b[0m, in \u001b[0;36mWorkflow.run.<locals>._run_workflow\u001b[1;34m()\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mwrite_event_to_stream(StopEvent())\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mwrite_event_to_stream(StopEvent())\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:253\u001b[0m, in \u001b[0;36mWorkflow._start.<locals>._task\u001b[1;34m(name, queue, step, config)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mretry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 253\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[0;32m    254\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in step \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     delay \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mretry_policy\u001b[38;5;241m.\u001b[39mnext(\n\u001b[0;32m    258\u001b[0m         retry_start_at \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mtime(), attempts, e\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[1;31mWorkflowRuntimeError\u001b[0m: Error in step 'query': error reading llm response: read tcp 127.0.0.1:51194->127.0.0.1:50061: wsarecv: An existing connection was forcibly closed by the remote host."
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# run a query\n",
    "res = await wf.run(\n",
    "    query_str=\"How can Pittsburgh become a startup hub, and what are the two types of moderates?\",\n",
    "    query_eng=result[\"query_engine\"],\n",
    ")\n",
    "display(Markdown(str(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workflow_longrag.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muthu\\AppData\\Local\\Temp\\ipykernel_31456\\1878300089.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) draw_all_possible_flows. (Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.)\n",
      "  draw_all_possible_flows(wf, filename=\"workflow_longrag.html\")\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(wf, filename=\"workflow_longrag.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
