{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joke in question is a play on words, using the nautical term \"arrrr\" to create a pun. Here's a thorough analysis and critique of the joke:\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "1. **Wordplay**: The use of \"arrrr\" as a verb (to argue) is a clever play on words. It's a common sound associated with pirates, but in this context, it's used to create a pun.\n",
      "2. **Simple and easy to understand**: The joke is straightforward and doesn't require complex setup or inside knowledge. This makes it accessible to a wide audience.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "1. **Predictability**: The punchline is fairly predictable, which can make the joke feel less surprising or impactful. A more unexpected twist might have made the joke more memorable.\n",
      "2. **Lack of originality**: The \"arrrr\" pun has been used in many other jokes and memes before this one. While it's still a clever play on words, it doesn't bring anything new to the table.\n",
      "3. **Limited humor scope**: The joke relies heavily on wordplay, which might not appeal to everyone. Some people might find it cheesy or annoying.\n",
      "\n",
      "**Critique:**\n",
      "\n",
      "While the joke is well-crafted and easy to understand, it's ultimately a fairly simple and predictable play on words. To make it more effective, the punchline could be made more unexpected or surprising. For example:\n",
      "\n",
      "\"Why did the pirate quit his job?\n",
      "Because he was sick of all the 'treason'!\"\n",
      "\n",
      "This revised version uses a different wordplay (using \"treason\" to reference both the act of betraying one's country and the idea of being disloyal in a workplace) that might be more memorable and impactful.\n",
      "\n",
      "**Suggestions for improvement:**\n",
      "\n",
      "1. **Add more context**: Consider adding more setup or context to make the joke feel more substantial. For example, \"Why did the pirate quit his job after 20 years at sea?\"\n",
      "2. **Make the punchline more unexpected**: Try to subvert expectations by using a different wordplay or twist.\n",
      "3. **Experiment with delivery**: Consider delivering the joke in a way that adds to its humor, such as with a silly voice or gesture.\n",
      "\n",
      "Overall, while the original joke is well-crafted and easy to understand, it's ultimately a fairly simple play on words that might not resonate with everyone. With some tweaks and refinements, it could be made more memorable and impactful.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "\n",
    "# `pip install llama-index-llms-openai` if you don't already have it\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "class JokeEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "\n",
    "class JokeFlow(Workflow):\n",
    "    # llm = OpenAI()\n",
    "    llm = ollama_llm\n",
    "\n",
    "    @step\n",
    "    async def generate_joke(self, ev: StartEvent) -> JokeEvent:\n",
    "        topic = ev.topic\n",
    "\n",
    "        prompt = f\"Write your best joke about {topic}.\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "        return JokeEvent(joke=str(response))\n",
    "\n",
    "    @step\n",
    "    async def critique_joke(self, ev: JokeEvent) -> StopEvent:\n",
    "        joke = ev.joke\n",
    "\n",
    "        prompt = f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "        return StopEvent(result=str(response))\n",
    "\n",
    "\n",
    "w = JokeFlow(timeout=60, verbose=False)\n",
    "result = await w.run(topic=\"pirates\")\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joke_workflow.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muthu\\AppData\\Local\\Temp\\ipykernel_28780\\363681527.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) draw_all_possible_flows. (Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.)\n",
      "  draw_all_possible_flows(w, filename=\"joke_workflow.html\")\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w, filename=\"joke_workflow.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
