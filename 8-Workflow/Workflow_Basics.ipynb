{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-llms-openai in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (0.2.16)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.7 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-llms-openai) (0.11.23)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-llms-openai) (1.54.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.10.8)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2024.9.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (0.7.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.17.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->llama-index-llms-openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.23.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (23.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (0.11.23)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.4 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.3.4)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.23 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.11.23)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.2.5)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.9.48.post4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.10 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.2.16)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.2.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.4.0,>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.54.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (3.10.8)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2024.9.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (3.4.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.16.0)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index) (0.1.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (5.1.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-readers-llama-parse>=0.3.0->llama-index) (0.5.13)\n",
      "Requirement already satisfied: click in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (2.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from click->nltk>3.8.1->llama-index) (0.4.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (0.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-core in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (0.11.23)\n",
      "Collecting llama-index-core\n",
      "  Downloading llama_index_core-0.12.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: llama-index-llms-openai in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (0.2.16)\n",
      "Collecting llama-index-llms-openai\n",
      "  Downloading llama_index_llms_openai-0.3.10-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-utils-workflow\n",
      "  Downloading llama_index_utils_workflow-0.3.0-py3-none-any.whl.metadata (665 bytes)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (3.10.8)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (2024.9.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-core) (1.16.0)\n",
      "Collecting openai<2.0.0,>=1.57.1 (from llama-index-llms-openai)\n",
      "  Downloading openai-1.57.4-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: pyvis<0.4.0,>=0.3.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from llama-index-utils-workflow) (0.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.17.1)\n",
      "Requirement already satisfied: click in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (2024.11.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from openai<2.0.0,>=1.57.1->llama-index-llms-openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from openai<2.0.0,>=1.57.1->llama-index-llms-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from openai<2.0.0,>=1.57.1->llama-index-llms-openai) (0.7.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from openai<2.0.0,>=1.57.1->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpx->llama-index-core) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core) (2.23.4)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (8.27.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.1.4)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (4.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from dataclasses-json->llama-index-core) (3.23.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (5.14.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from jinja2>=2.9.6->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (23.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.13)\n",
      "Requirement already satisfied: executing in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\muthu\\.conda\\envs\\llamaindex\\lib\\site-packages (from asttokens->stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (1.16.0)\n",
      "Downloading llama_index_core-0.12.5-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.8/1.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 8.5 MB/s eta 0:00:00\n",
      "Downloading llama_index_llms_openai-0.3.10-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_utils_workflow-0.3.0-py3-none-any.whl (2.8 kB)\n",
      "Downloading openai-1.57.4-py3-none-any.whl (390 kB)\n",
      "Installing collected packages: openai, llama-index-core, llama-index-llms-openai, llama-index-utils-workflow\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.54.3\n",
      "    Uninstalling openai-1.54.3:\n",
      "      Successfully uninstalled openai-1.54.3\n",
      "  Attempting uninstall: llama-index-core\n",
      "    Found existing installation: llama-index-core 0.11.23\n",
      "    Uninstalling llama-index-core-0.11.23:\n",
      "      Successfully uninstalled llama-index-core-0.11.23\n",
      "  Attempting uninstall: llama-index-llms-openai\n",
      "    Found existing installation: llama-index-llms-openai 0.2.16\n",
      "    Uninstalling llama-index-llms-openai-0.2.16:\n",
      "      Successfully uninstalled llama-index-llms-openai-0.2.16\n",
      "Successfully installed llama-index-core-0.12.5 llama-index-llms-openai-0.3.10 llama-index-utils-workflow-0.3.0 openai-1.57.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index 0.11.23 requires llama-index-core<0.12.0,>=0.11.23, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index 0.11.23 requires llama-index-llms-openai<0.3.0,>=0.2.10, but you have llama-index-llms-openai 0.3.10 which is incompatible.\n",
      "llama-index-agent-openai 0.3.4 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index-agent-openai 0.3.4 requires llama-index-llms-openai<0.3.0,>=0.2.9, but you have llama-index-llms-openai 0.3.10 which is incompatible.\n",
      "llama-index-cli 0.3.1 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index-cli 0.3.1 requires llama-index-llms-openai<0.3.0,>=0.2.0, but you have llama-index-llms-openai 0.3.10 which is incompatible.\n",
      "llama-index-embeddings-ollama 0.3.1 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index-embeddings-openai 0.2.5 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index-indices-managed-llama-cloud 0.4.0 requires llama-index-core<0.12.0,>=0.11.13.post1, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index-llms-ollama 0.3.6 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index-multi-modal-llms-openai 0.2.3 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index-multi-modal-llms-openai 0.2.3 requires llama-index-llms-openai<0.3.0,>=0.2.11, but you have llama-index-llms-openai 0.3.10 which is incompatible.\n",
      "llama-index-program-openai 0.2.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index-program-openai 0.2.0 requires llama-index-llms-openai<0.3.0,>=0.2.0, but you have llama-index-llms-openai 0.3.10 which is incompatible.\n",
      "llama-index-question-gen-openai 0.2.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index-question-gen-openai 0.2.0 requires llama-index-llms-openai<0.3.0,>=0.2.0, but you have llama-index-llms-openai 0.3.10 which is incompatible.\n",
      "llama-index-readers-file 0.3.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index-readers-google 0.4.3 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index-readers-llama-parse 0.3.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "llama-index-vector-stores-chroma 0.3.0 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.5 which is incompatible.\n",
      "open-webui 0.3.35 requires chromadb==0.5.9, but you have chromadb 0.5.20 which is incompatible.\n",
      "open-webui 0.3.35 requires pypdf==4.3.1, but you have pypdf 5.1.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-openai\n",
    "!pip install llama-index\n",
    "!pip install --upgrade llama-index-core llama-index-llms-openai llama-index-utils-workflow\n",
    "\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\Muthu\\\\.conda\\\\envs\\\\llamaindex\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "load_verify_locations cafile='C:\\\\Users\\\\Muthu\\\\.conda\\\\envs\\\\llamaindex\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\Muthu\\\\.conda\\\\envs\\\\llamaindex\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "load_verify_locations cafile='C:\\\\Users\\\\Muthu\\\\.conda\\\\envs\\\\llamaindex\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\Muthu\\\\.conda\\\\envs\\\\llamaindex\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "load_verify_locations cafile='C:\\\\Users\\\\Muthu\\\\.conda\\\\envs\\\\llamaindex\\\\Library\\\\ssl\\\\cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaGenerator(Workflow):\n",
    "    @step\n",
    "    async def generate(self, ev: StartEvent) -> StopEvent:\n",
    "        response = await llm.acomplete(ev.query)  # Query Ollama asynchronously\n",
    "        return StopEvent(result=str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.instrumentation.dispatcher:Failed to reset active_span_id: <Token var=<ContextVar name='active_span_id' default=None at 0x000001E3C0E8A660> at 0x000001E3A9141B00> was created in a different Context\n",
      "Failed to reset active_span_id: <Token var=<ContextVar name='active_span_id' default=None at 0x000001E3C0E8A660> at 0x000001E3A9141B00> was created in a different Context\n"
     ]
    },
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'generate': name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:247\u001b[0m, in \u001b[0;36mWorkflow._start.<locals>._task\u001b[1;34m(name, queue, step, config)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     new_ev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# exit the retrying loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:367\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 367\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mOllamaGenerator.generate\u001b[1;34m(self, ev)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;129m@step\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, ev: StartEvent) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StopEvent:\n\u001b[1;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m llm\u001b[38;5;241m.\u001b[39macomplete(ev\u001b[38;5;241m.\u001b[39mquery)  \u001b[38;5;66;03m# Query Ollama asynchronously\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StopEvent(result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(response))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mWorkflowRuntimeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m w \u001b[38;5;241m=\u001b[39m OllamaGenerator(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m w\u001b[38;5;241m.\u001b[39mrun(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms LlamaIndex?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\asyncio\\futures.py:287\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\asyncio\\tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 349\u001b[0m         future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:438\u001b[0m, in \u001b[0;36mWorkflow.run.<locals>._run_workflow\u001b[1;34m()\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mwrite_event_to_stream(StopEvent())\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mwrite_event_to_stream(StopEvent())\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\Muthu\\.conda\\envs\\llamaindex\\Lib\\site-packages\\llama_index\\core\\workflow\\workflow.py:253\u001b[0m, in \u001b[0;36mWorkflow._start.<locals>._task\u001b[1;34m(name, queue, step, config)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mretry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 253\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[0;32m    254\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in step \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     delay \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mretry_policy\u001b[38;5;241m.\u001b[39mnext(\n\u001b[0;32m    258\u001b[0m         retry_start_at \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mtime(), attempts, e\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[1;31mWorkflowRuntimeError\u001b[0m: Error in step 'generate': name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "w = OllamaGenerator(timeout=10, verbose=False)\n",
    "result = await w.run(query=\"What's LlamaIndex?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='C:\\\\Users\\\\Muthu\\\\.conda\\\\envs\\\\llamaindex\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "load_verify_locations cafile='C:\\\\Users\\\\Muthu\\\\.conda\\\\envs\\\\llamaindex\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='localhost' port=11434 local_address=None timeout=30.0 socket_options=None\n",
      "connect_tcp.started host='localhost' port=11434 local_address=None timeout=30.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001E3AA391190>\n",
      "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001E3AA391190>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 15 Dec 2024 18:18:40 GMT'), (b'Content-Length', b'1757')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 15 Dec 2024 18:18:40 GMT'), (b'Content-Length', b'1757')])\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:llama_index.core.instrumentation.dispatcher:Failed to reset active_span_id: <Token var=<ContextVar name='active_span_id' default=None at 0x000001E3C0E8A660> at 0x000001E3A91425C0> was created in a different Context\n",
      "Failed to reset active_span_id: <Token var=<ContextVar name='active_span_id' default=None at 0x000001E3C0E8A660> at 0x000001E3A91425C0> was created in a different Context\n",
      "Cosine similarity is a measure of the similarity between two vectors in a multi-dimensional space. It is defined as the dot product of the two vectors divided by the magnitude (length) of each vector.\n",
      "\n",
      "Given two vectors A and B, the cosine similarity is calculated as:\n",
      "\n",
      "cos(A, B) = A  B / |A| |B|\n",
      "\n",
      "where A  B is the dot product of the two vectors, and |A| and |B| are the magnitudes (or lengths) of the two vectors.\n",
      "\n",
      "The cosine similarity ranges from -1 to 1, where:\n",
      "\n",
      "* cos(A, B) = 1 means that the two vectors are identical\n",
      "* cos(A, B) = 0 means that the two vectors are orthogonal (perpendicular)\n",
      "* cos(A, B) < 0 means that one vector is a negative version of the other\n",
      "\n",
      "Cosine similarity is often used in natural language processing, computer vision, and machine learning to measure the similarity between text documents, images, or other types of data.\n",
      "\n",
      "For example, if you have two sentences:\n",
      "\n",
      " Sentence A: \"The sun is shining brightly.\"\n",
      " Sentence B: \"The sky is a beautiful blue.\"\n",
      "\n",
      "You can calculate the cosine similarity between these two sentences by converting them into vectors (e.g., using word embeddings like Word2Vec) and then computing the dot product. The resulting value will indicate how similar the two sentences are.\n",
      "\n",
      "In general, cosine similarity is a useful metric for measuring the similarity between data points in high-dimensional spaces, where traditional distance metrics may not be effective.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "from llama_index.llms.ollama import Ollama  # Ollama integration\n",
    "import random\n",
    "\n",
    "# Define the Ollama instance globally or as part of the workflow\n",
    "class OllamaGenerator(Workflow):\n",
    "    def __init__(self, timeout: int, verbose: bool = False):\n",
    "        super().__init__(timeout=timeout, verbose=verbose)\n",
    "        self.llm = Ollama(\n",
    "            base_url=\"http://localhost:11434\",  # Your local Ollama setup\n",
    "            model=\"llama3.2:latest\",            # The preferred model\n",
    "            temperature=0.1                     # Controlled temperature for deterministic results\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def generate(self, ev: StartEvent) -> StopEvent:\n",
    "        response = await self.llm.acomplete(ev.query)  # Use the llm defined in the class\n",
    "        return StopEvent(result=str(response))\n",
    "\n",
    "# Initialize and run the workflow\n",
    "w = OllamaGenerator(timeout=10, verbose=False)\n",
    "result = await w.run(query=\"What's cosine similarity?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muthu\\AppData\\Local\\Temp\\ipykernel_29744\\1234723504.py:1: DeprecationWarning: Call to deprecated function (or staticmethod) draw_all_possible_flows. (Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.)\n",
      "  draw_all_possible_flows(OllamaGenerator, filename=\"trivial_workflow.html\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivial_workflow.html\n"
     ]
    }
   ],
   "source": [
    "draw_all_possible_flows(OllamaGenerator, filename=\"trivial_workflow.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=['../data/paul_graham_essay3.txt']).load_data()\n",
    "# documents = SimpleDirectoryReader(input_files=['../data/2022 Q3 AAPL.pdf']).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# vector_index = VectorStoreIndex.from_documents(documents, embed_model=ollama_embedding)\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt_tmpl_str = \"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Please write the answer in the style of {tone_name}\n",
    "Query: {query_str}\n",
    "Answer: \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PromptTemplate object\n",
    "prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "# Apply partial formatting to set the tone_name\n",
    "partial_prompt_tmpl = prompt_tmpl.partial_format(tone_name=\"Shakespeare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = partial_prompt_tmpl.format(\n",
    "    context_str=\"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters\",\n",
    "    query_str=\"How many params does Llama 2 have?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = vector_index.as_query_engine(\n",
    "    llm=ollama_llm\n",
    ").query(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nResponse from the Query Engine:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
