{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom events\n",
    "class JokeEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "class ProgressEvent(Event):\n",
    "    msg: str\n",
    "\n",
    "class CritiqueEvent(Event):\n",
    "    joke: str\n",
    "    critique: str\n",
    "\n",
    "class FinalResultEvent(Event):\n",
    "    result: str\n",
    "\n",
    "# Define the workflow\n",
    "class JokeFlow(Workflow):\n",
    "    llm = ollama_llm\n",
    "\n",
    "    @step\n",
    "    async def generate_joke(self, ctx: Context, ev: StartEvent) -> JokeEvent:\n",
    "        \"\"\"\n",
    "        Generate a joke and manually dispatch and return the JokeEvent.\n",
    "        \"\"\"\n",
    "        topic = ev.topic\n",
    "\n",
    "        # Generate the joke\n",
    "        prompt = f\"Write your best joke about {topic}.\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "\n",
    "        joke = str(response)\n",
    "        await ctx.set(\"joke\", joke)\n",
    "\n",
    "        # Manually dispatch the JokeEvent\n",
    "        ctx.send_event(JokeEvent(joke=joke))\n",
    "\n",
    "        # Stream progress\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Joke generated and dispatched.\"))\n",
    "\n",
    "        # Also return the JokeEvent to satisfy workflow validation\n",
    "        return JokeEvent(joke=joke)\n",
    "\n",
    "    @step\n",
    "    async def critique_joke(self, ctx: Context, ev: JokeEvent) -> CritiqueEvent:\n",
    "        \"\"\"\n",
    "        Critique the joke and return a CritiqueEvent.\n",
    "        \"\"\"\n",
    "        joke = ev.joke\n",
    "        prompt = f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "\n",
    "        critique = str(response)\n",
    "        await ctx.set(\"critique\", critique)\n",
    "\n",
    "        # Stream progress\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Critique generated.\"))\n",
    "\n",
    "        return CritiqueEvent(joke=joke, critique=critique)\n",
    "\n",
    "    @step\n",
    "    async def finalize_result(self, ctx: Context, ev: CritiqueEvent) -> FinalResultEvent:\n",
    "        \"\"\"\n",
    "        Combine the joke and critique into a final result and stream it.\n",
    "        \"\"\"\n",
    "        joke = ev.joke\n",
    "        critique = ev.critique\n",
    "\n",
    "        result = f\"Joke: {joke}\\n\\nCritique: {critique}\"\n",
    "        await ctx.set(\"final_result\", result)\n",
    "\n",
    "        # Stream the final result\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Final result prepared.\"))\n",
    "\n",
    "        return FinalResultEvent(result=result)\n",
    "\n",
    "    @step\n",
    "    async def complete_workflow(self, ctx: Context, ev: FinalResultEvent) -> StopEvent:\n",
    "        \"\"\"\n",
    "        Stop the workflow with the final result.\n",
    "        \"\"\"\n",
    "        result = ev.result\n",
    "        return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow\n",
    "w = JokeFlow(timeout=120, verbose=True)\n",
    "\n",
    "async def main():\n",
    "    # Start the workflow with a topic\n",
    "    handler = w.run(topic=\"pirates\")\n",
    "\n",
    "    # Stream events\n",
    "    print(\"Streaming events in real-time:\")\n",
    "    async for event in handler.stream_events():\n",
    "        if isinstance(event, ProgressEvent):\n",
    "            print(f\"[Progress]: {event.msg}\")\n",
    "\n",
    "    # Wait for the final result\n",
    "    final_result = await handler\n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(str(final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming events in real-time:\n",
      "Running step generate_joke\n",
      "Step generate_joke produced event JokeEvent\n",
      "Running step critique_joke\n",
      "[Progress]: Joke generated and dispatched.\n",
      "Running step critique_joke\n",
      "Step critique_joke produced event CritiqueEvent\n",
      "[Progress]: Critique generated.\n",
      "Running step finalize_result\n",
      "Step finalize_result produced event FinalResultEvent\n",
      "[Progress]: Final result prepared.\n",
      "Running step complete_workflow\n",
      "Step complete_workflow produced event StopEvent\n",
      "\n",
      "Final Result:\n",
      "Joke: Why did the pirate quit his job?\n",
      "\n",
      "Because he was sick of all the arrrr-guments! (get it?)\n",
      "\n",
      "Critique: The joke in question is a play on words, using the nautical term \"arrrr\" to create a pun. Here's a thorough analysis and critique of the joke:\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "1. **Wordplay**: The use of \"arrrr\" as a verb, implying disagreement or argument, is a clever play on words. It's a common phrase associated with pirates, but in this context, it takes on a new meaning.\n",
      "2. **Simple and concise**: The joke is easy to understand, and the punchline is quick to deliver.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "1. **Predictability**: The joke relies heavily on the listener being familiar with the pirate theme and the word \"arrrr.\" This makes it less likely to surprise or delight the audience.\n",
      "2. **Lack of originality**: The pun itself isn't particularly new or creative. It's a common type of joke, and the execution is straightforward.\n",
      "3. **Limited humor**: The joke may elicit a chuckle from some, but others might find it too simple or cheesy. The humor relies on a relatively narrow audience being familiar with pirate culture.\n",
      "\n",
      "**Critique:**\n",
      "\n",
      "While the joke has some strengths, its predictability and lack of originality hold it back. To make this joke more effective, consider adding an unexpected twist or layer to the punchline. For example:\n",
      "\n",
      "\"Why did the pirate quit his job?\n",
      "Because he was sick of all the arrrr-guments... but then he realized he could just walk the plank!\"\n",
      "\n",
      "This revised version adds a new layer of humor by introducing a second punchline that subverts expectations and creates a more unexpected connection between the setup and the payoff.\n",
      "\n",
      "**Suggestions for improvement:**\n",
      "\n",
      "1. **Add more complexity**: Consider adding an additional layer to the joke, such as a secondary pun or a clever twist on the wordplay.\n",
      "2. **Use more vivid language**: Instead of relying on a single word like \"arrrr,\" try using more descriptive language to create a richer atmosphere and make the joke more engaging.\n",
      "3. **Experiment with delivery**: Consider varying the tone, pace, or delivery of the joke to add more emphasis or surprise to the punchline.\n",
      "\n",
      "Overall, while the original joke has some strengths, it could benefit from additional complexity, creativity, and attention to detail to elevate it to a more memorable and effective comedic experience.\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joke_workflow_context.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muthu\\AppData\\Local\\Temp\\ipykernel_20048\\87609920.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) draw_all_possible_flows. (Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.)\n",
      "  draw_all_possible_flows(w, filename=\"joke_workflow_context.html\")\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w, filename=\"joke_workflow_context.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
