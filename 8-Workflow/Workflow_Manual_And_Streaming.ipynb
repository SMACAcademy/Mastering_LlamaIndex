{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Define custom events\n",
    "class JokeEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "class ProgressEvent(Event):\n",
    "    msg: str\n",
    "\n",
    "class CritiqueEvent(Event):\n",
    "    joke: str\n",
    "    critique: str\n",
    "\n",
    "class FinalResultEvent(Event):\n",
    "    result: str\n",
    "\n",
    "# Define the workflow\n",
    "class JokeFlow(Workflow):\n",
    "    llm = ollama_llm\n",
    "\n",
    "    @step\n",
    "    async def generate_joke(self, ctx: Context, ev: StartEvent) -> JokeEvent:\n",
    "        \"\"\"\n",
    "        Generate a joke and manually dispatch and return the JokeEvent.\n",
    "        \"\"\"\n",
    "        topic = ev.topic\n",
    "\n",
    "        # Generate the joke\n",
    "        prompt = f\"Write your best joke about {topic}.\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "\n",
    "        joke = str(response)\n",
    "        await ctx.set(\"joke\", joke)\n",
    "\n",
    "        # Manually dispatch the JokeEvent\n",
    "        ctx.send_event(JokeEvent(joke=joke))\n",
    "\n",
    "        # Stream progress\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Joke generated and dispatched.\"))\n",
    "\n",
    "        # Also return the JokeEvent to satisfy workflow validation\n",
    "        return JokeEvent(joke=joke)\n",
    "\n",
    "    @step\n",
    "    async def critique_joke(self, ctx: Context, ev: JokeEvent) -> CritiqueEvent:\n",
    "        \"\"\"\n",
    "        Critique the joke and return a CritiqueEvent.\n",
    "        \"\"\"\n",
    "        joke = ev.joke\n",
    "        prompt = f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "\n",
    "        critique = str(response)\n",
    "        await ctx.set(\"critique\", critique)\n",
    "\n",
    "        # Stream progress\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Critique generated.\"))\n",
    "\n",
    "        return CritiqueEvent(joke=joke, critique=critique)\n",
    "\n",
    "    @step\n",
    "    async def finalize_result(self, ctx: Context, ev: CritiqueEvent) -> FinalResultEvent:\n",
    "        \"\"\"\n",
    "        Combine the joke and critique into a final result and stream it.\n",
    "        \"\"\"\n",
    "        joke = ev.joke\n",
    "        critique = ev.critique\n",
    "\n",
    "        result = f\"Joke: {joke}\\n\\nCritique: {critique}\"\n",
    "        await ctx.set(\"final_result\", result)\n",
    "\n",
    "        # Stream the final result\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Final result prepared.\"))\n",
    "\n",
    "        return FinalResultEvent(result=result)\n",
    "\n",
    "    @step\n",
    "    async def complete_workflow(self, ctx: Context, ev: FinalResultEvent) -> StopEvent:\n",
    "        \"\"\"\n",
    "        Stop the workflow with the final result.\n",
    "        \"\"\"\n",
    "        result = ev.result\n",
    "        return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow\n",
    "w = JokeFlow(timeout=120, verbose=True)\n",
    "\n",
    "async def main():\n",
    "    # Start the workflow with a topic\n",
    "    handler = w.run(topic=\"pirates\")\n",
    "\n",
    "    # Stream events\n",
    "    print(\"Streaming events in real-time:\")\n",
    "    async for event in handler.stream_events():\n",
    "        if isinstance(event, ProgressEvent):\n",
    "            print(f\"[Progress]: {event.msg}\")\n",
    "\n",
    "    # Wait for the final result\n",
    "    final_result = await handler\n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(str(final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming events in real-time:\n",
      "Running step generate_joke\n",
      "Step generate_joke produced event JokeEvent\n",
      "Running step critique_joke\n",
      "[Progress]: Joke generated and dispatched.\n",
      "Running step critique_joke\n",
      "Step critique_joke produced event CritiqueEvent\n",
      "[Progress]: Critique generated.\n",
      "Running step finalize_result\n",
      "Step finalize_result produced event FinalResultEvent\n",
      "[Progress]: Final result prepared.\n",
      "Running step complete_workflow\n",
      "Step complete_workflow produced event StopEvent\n",
      "\n",
      "Final Result:\n",
      "Joke: Why did the pirate quit his job?\n",
      "\n",
      "Because he was sick of all the arrrr-guments! (get it?)\n",
      "\n",
      "Critique: The joke in question is a play on words, relying on the multiple meanings of \"arrrr\" to create a pun. Here's a thorough analysis and critique of the joke:\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "1. **Wordplay**: The use of \"arrrr\" as both a pirate-themed sound effect and a homophone for \"arguments\" is clever and attention-grabbing.\n",
      "2. **Simple setup**: The joke has a straightforward setup, making it easy to understand what's happening in the story.\n",
      "3. **Quick punchline**: The punchline is delivered quickly, which can create a sense of surprise and delight.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "1. **Predictability**: The use of wordplay as a punchline can be predictable, especially for those familiar with pirate-themed jokes. This may reduce the joke's impact.\n",
      "2. **Lack of originality**: The \"arrrr\" pun is not particularly new or unique. It's been used in many other jokes and memes before this one.\n",
      "3. **Limited depth**: The joke relies on a single wordplay, which doesn't provide much depth or complexity to the humor.\n",
      "4. **Forced delivery**: The joke may feel forced or contrived, especially if the listener is not familiar with pirate-themed jokes.\n",
      "\n",
      "**Critique:**\n",
      "\n",
      "While the joke has some clever elements, it ultimately feels like a shallow play on words. To make this joke more effective, consider adding more depth and complexity to the humor. Here are some suggestions:\n",
      "\n",
      "1. **Add more context**: Provide more background information about why the pirate quit his job or what kind of \"arrrr-guments\" he was dealing with.\n",
      "2. **Make it more unexpected**: Consider subverting expectations by making the punchline less obvious or using a different type of wordplay.\n",
      "3. **Emphasize the absurdity**: Exaggerate the situation or the pirate's frustration to make the joke feel more ridiculous and humorous.\n",
      "\n",
      "**Alternative versions:**\n",
      "\n",
      "Here are some alternative versions of the joke that build upon the original idea:\n",
      "\n",
      "* Why did the pirate quit his job? He was sick of all the swashbuckling bureaucracy!\n",
      "* Why did the pirate quit his job? He couldn't take the constant sea-sonal changes in management!\n",
      "* Why did the pirate quit his job? He was fed up with all the anchor-ing to outdated policies!\n",
      "\n",
      "These versions add more depth and complexity to the humor, making them feel more original and engaging.\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muthu\\AppData\\Local\\Temp\\ipykernel_4448\\87609920.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) draw_all_possible_flows. (Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.)\n",
      "  draw_all_possible_flows(w, filename=\"joke_workflow_context.html\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joke_workflow_context.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w, filename=\"joke_workflow_manual.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
