{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    "    InputRequiredEvent,\n",
    "    HumanResponseEvent,\n",
    ")\n",
    "\n",
    "from llama_index.core.workflow.retry_policy import ConstantDelayRetryPolicy\n",
    "\n",
    "# Define custom events\n",
    "class JokeEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "class ProgressEvent(Event):\n",
    "    msg: str\n",
    "\n",
    "class CritiqueEvent(Event):\n",
    "    joke: str\n",
    "    critique: str\n",
    "\n",
    "class FinalResultEvent(Event):\n",
    "    result: str\n",
    "\n",
    "# Define the workflow\n",
    "class JokeFlow(Workflow):\n",
    "    llm = ollama_llm\n",
    "\n",
    "    @step\n",
    "    async def generate_joke(self, ctx: Context, ev: StartEvent) -> JokeEvent:\n",
    "        topic = ev.topic\n",
    "        prompt = f\"Write your best joke about {topic}.\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "\n",
    "        joke = str(response)\n",
    "        await ctx.set(\"joke\", joke)\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Joke generated.\"))\n",
    "\n",
    "        return JokeEvent(joke=joke)\n",
    "\n",
    "    @step(retry_policy=ConstantDelayRetryPolicy(delay=5, maximum_attempts=3))\n",
    "    async def critique_joke(self, ctx: Context, ev: JokeEvent) -> InputRequiredEvent:\n",
    "        joke = ev.joke\n",
    "        prompt = f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "\n",
    "        try:\n",
    "            response = await self.llm.acomplete(prompt)\n",
    "        except Exception as e:\n",
    "            ctx.write_event_to_stream(ProgressEvent(msg=f\"Critique failed: {e}\"))\n",
    "            raise e\n",
    "\n",
    "        critique = str(response)\n",
    "        await ctx.set(\"critique\", critique)\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Critique generated.\"))\n",
    "\n",
    "        return InputRequiredEvent(prefix=f\"Review the critique and approve: {critique}\")\n",
    "\n",
    "    @step\n",
    "    async def human_validation(self, ctx: Context, ev: HumanResponseEvent) -> CritiqueEvent:\n",
    "        approval = ev.response\n",
    "\n",
    "        if approval.lower() not in [\"yes\", \"approved\"]:\n",
    "            ctx.write_event_to_stream(ProgressEvent(msg=\"Critique rejected.\"))\n",
    "            raise ValueError(\"Critique was rejected by the user.\")\n",
    "\n",
    "        joke = await ctx.get(\"joke\")\n",
    "        critique = await ctx.get(\"critique\")\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Critique approved by user.\"))\n",
    "\n",
    "        return CritiqueEvent(joke=joke, critique=critique)\n",
    "\n",
    "    @step\n",
    "    async def finalize_result(self, ctx: Context, ev: CritiqueEvent) -> FinalResultEvent:\n",
    "        joke = ev.joke\n",
    "        critique = ev.critique\n",
    "\n",
    "        result = f\"Joke: {joke}\\n\\nCritique: {critique}\"\n",
    "        await ctx.set(\"final_result\", result)\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Final result prepared.\"))\n",
    "\n",
    "        return FinalResultEvent(result=result)\n",
    "\n",
    "    @step\n",
    "    async def complete_workflow(self, ctx: Context, ev: FinalResultEvent) -> StopEvent:\n",
    "        result = ev.result\n",
    "        return StopEvent(result=result)\n",
    "\n",
    "    # **Placeholder Steps for Visualization**\n",
    "    @step\n",
    "    async def visualize_input_required(self, ev: InputRequiredEvent) -> HumanResponseEvent:\n",
    "        \"\"\"\n",
    "        Placeholder step to link InputRequiredEvent to HumanResponseEvent.\n",
    "        \"\"\"\n",
    "        return HumanResponseEvent(response=\"yes\")\n",
    "\n",
    "    @step\n",
    "    async def visualize_human_response(self, ev: HumanResponseEvent) -> None:\n",
    "        \"\"\"\n",
    "        Placeholder step to ensure HumanResponseEvent is recognized.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow\n",
    "w = JokeFlow(timeout=120, verbose=True)\n",
    "\n",
    "async def main():\n",
    "    # Start the workflow with a topic\n",
    "    handler = w.run(topic=\"pirates\")\n",
    "\n",
    "    # Stream events\n",
    "    print(\"Streaming events in real-time:\")\n",
    "    async for event in handler.stream_events():\n",
    "        if isinstance(event, ProgressEvent):\n",
    "            print(f\"[Progress]: {event.msg}\")\n",
    "        elif isinstance(event, InputRequiredEvent):\n",
    "            # Simulate user input (replace with `input()` for real interaction)\n",
    "            # user_response = \"yes\"  # Simulating approval\n",
    "            user_response = input(event.prefix)\n",
    "            handler.ctx.send_event(HumanResponseEvent(response=user_response))\n",
    "\n",
    "    # Wait for the final result\n",
    "    final_result = await handler\n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(str(final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w, filename=\"joke_workflow_human_loop.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
