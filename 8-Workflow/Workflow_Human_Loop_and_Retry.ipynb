{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    "    InputRequiredEvent,\n",
    "    HumanResponseEvent,\n",
    ")\n",
    "\n",
    "from llama_index.core.workflow.retry_policy import ConstantDelayRetryPolicy\n",
    "\n",
    "# Define custom events\n",
    "class JokeEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "class ProgressEvent(Event):\n",
    "    msg: str\n",
    "\n",
    "class CritiqueEvent(Event):\n",
    "    joke: str\n",
    "    critique: str\n",
    "\n",
    "class FinalResultEvent(Event):\n",
    "    result: str\n",
    "\n",
    "# Define the workflow\n",
    "class JokeFlow(Workflow):\n",
    "    llm = ollama_llm\n",
    "\n",
    "    @step\n",
    "    async def generate_joke(self, ctx: Context, ev: StartEvent) -> JokeEvent:\n",
    "        topic = ev.topic\n",
    "        prompt = f\"Write your best joke about {topic}.\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "\n",
    "        joke = str(response)\n",
    "        await ctx.set(\"joke\", joke)\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Joke generated.\"))\n",
    "\n",
    "        return JokeEvent(joke=joke)\n",
    "\n",
    "    @step(retry_policy=ConstantDelayRetryPolicy(delay=5, maximum_attempts=3))\n",
    "    async def critique_joke(self, ctx: Context, ev: JokeEvent) -> InputRequiredEvent:\n",
    "        joke = ev.joke\n",
    "        prompt = f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "\n",
    "        try:\n",
    "            response = await self.llm.acomplete(prompt)\n",
    "        except Exception as e:\n",
    "            ctx.write_event_to_stream(ProgressEvent(msg=f\"Critique failed: {e}\"))\n",
    "            raise e\n",
    "\n",
    "        critique = str(response)\n",
    "        await ctx.set(\"critique\", critique)\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Critique generated.\"))\n",
    "\n",
    "        return InputRequiredEvent(prefix=f\"Review the critique and approve: {critique}\")\n",
    "\n",
    "    @step\n",
    "    async def human_validation(self, ctx: Context, ev: HumanResponseEvent) -> CritiqueEvent:\n",
    "        approval = ev.response\n",
    "\n",
    "        if approval.lower() not in [\"yes\", \"approved\"]:\n",
    "            ctx.write_event_to_stream(ProgressEvent(msg=\"Critique rejected.\"))\n",
    "            raise ValueError(\"Critique was rejected by the user.\")\n",
    "\n",
    "        joke = await ctx.get(\"joke\")\n",
    "        critique = await ctx.get(\"critique\")\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Critique approved by user.\"))\n",
    "\n",
    "        return CritiqueEvent(joke=joke, critique=critique)\n",
    "\n",
    "    @step\n",
    "    async def finalize_result(self, ctx: Context, ev: CritiqueEvent) -> FinalResultEvent:\n",
    "        joke = ev.joke\n",
    "        critique = ev.critique\n",
    "\n",
    "        result = f\"Joke: {joke}\\n\\nCritique: {critique}\"\n",
    "        await ctx.set(\"final_result\", result)\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Final result prepared.\"))\n",
    "\n",
    "        return FinalResultEvent(result=result)\n",
    "\n",
    "    @step\n",
    "    async def complete_workflow(self, ctx: Context, ev: FinalResultEvent) -> StopEvent:\n",
    "        result = ev.result\n",
    "        return StopEvent(result=result)\n",
    "\n",
    "    # **Placeholder Steps for Visualization**\n",
    "    @step\n",
    "    async def visualize_input_required(self, ev: InputRequiredEvent) -> HumanResponseEvent:\n",
    "        \"\"\"\n",
    "        Placeholder step to link InputRequiredEvent to HumanResponseEvent.\n",
    "        \"\"\"\n",
    "        return HumanResponseEvent(response=\"yes\")\n",
    "\n",
    "    @step\n",
    "    async def visualize_human_response(self, ev: HumanResponseEvent) -> None:\n",
    "        \"\"\"\n",
    "        Placeholder step to ensure HumanResponseEvent is recognized.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow\n",
    "w = JokeFlow(timeout=120, verbose=True)\n",
    "\n",
    "async def main():\n",
    "    # Start the workflow with a topic\n",
    "    handler = w.run(topic=\"pirates\")\n",
    "\n",
    "    # Stream events\n",
    "    print(\"Streaming events in real-time:\")\n",
    "    async for event in handler.stream_events():\n",
    "        if isinstance(event, ProgressEvent):\n",
    "            print(f\"[Progress]: {event.msg}\")\n",
    "        elif isinstance(event, InputRequiredEvent):\n",
    "            # Simulate user input (replace with `input()` for real interaction)\n",
    "            # user_response = \"yes\"  # Simulating approval\n",
    "            user_response = input(event.prefix)\n",
    "            handler.ctx.send_event(HumanResponseEvent(response=user_response))\n",
    "\n",
    "    # Wait for the final result\n",
    "    final_result = await handler\n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(str(final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming events in real-time:\n",
      "Running step generate_joke\n",
      "Step generate_joke produced event JokeEvent\n",
      "[Progress]: Joke generated.\n",
      "Running step critique_joke\n",
      "Step critique_joke produced event InputRequiredEvent\n",
      "[Progress]: Critique generated.\n",
      "Running step human_validation\n",
      "Step human_validation produced event CritiqueEvent\n",
      "Running step visualize_human_response\n",
      "Step visualize_human_response produced no event\n",
      "[Progress]: Critique approved by user.\n",
      "Running step finalize_result\n",
      "Step finalize_result produced event FinalResultEvent\n",
      "[Progress]: Final result prepared.\n",
      "Running step complete_workflow\n",
      "Step complete_workflow produced event StopEvent\n",
      "\n",
      "Final Result:\n",
      "Joke: Why did the pirate quit his job?\n",
      "\n",
      "Because he was sick of all the arrrr-guments! (get it?)\n",
      "\n",
      "Critique: The joke in question is a play on words, relying on the multiple meanings of \"arrrr\" to create a pun. Here's a thorough analysis and critique:\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "1. **Wordplay**: The use of \"arrrr\" as both a pirate-themed sound effect and a homophone for \"arguments\" is clever and unexpected. This type of wordplay can be effective in creating humor.\n",
      "2. **Simple setup**: The joke has a straightforward setup, making it easy to understand what's happening. The punchline is quick and doesn't require much context.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "1. **Predictability**: The joke relies on a fairly common pun, which may make it less surprising or original. Puns can be hit-or-miss, and this one might not land well with everyone.\n",
      "2. **Lack of surprise**: The connection between \"arrrr\" and \"arguments\" is relatively obvious, which reduces the surprise factor. A good joke often subverts expectations or creates a unexpected twist.\n",
      "3. **Limited depth**: The joke doesn't have much depth or complexity. It's a simple play on words that might not resonate with listeners who prefer more nuanced humor.\n",
      "4. **Overuse of pirate clichés**: While the use of \"arrrr\" is clever, it also relies on a familiar pirate trope (the pirate sound effect). This might make the joke feel like a shallow attempt to be funny rather than a genuine creative effort.\n",
      "\n",
      "**Critique:**\n",
      "\n",
      "While the joke has some strengths, its predictability and lack of surprise hold it back. A more effective joke would either subvert expectations or create a unexpected twist. Additionally, the use of pirate clichés might make the joke feel less original.\n",
      "\n",
      "To improve this joke, consider adding more complexity or depth to the setup or punchline. For example:\n",
      "\n",
      "* Add an unexpected layer: \"Why did the pirate quit his job? Because he was sick of all the arrrr-guments... and also because he wanted to find a new career that didn't involve so much 'treason'ous paperwork!\"\n",
      "* Use more clever wordplay: \"Why did the pirate quit his job? Because he was fed up with all the 'sea-rious' disagreements!\"\n",
      "\n",
      "By incorporating these elements, you can create a more engaging and memorable joke that will resonate with listeners.\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muthu\\AppData\\Local\\Temp\\ipykernel_31892\\767995495.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) draw_all_possible_flows. (Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.)\n",
      "  draw_all_possible_flows(w, filename=\"joke_workflow_human_loop.html\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joke_workflow_human_loop.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w, filename=\"joke_workflow_human_loop.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
