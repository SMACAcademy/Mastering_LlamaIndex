{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=['../data/paul_graham_essay3.txt']).load_data()\n",
    "# documents = SimpleDirectoryReader(input_files=['../data/2022 Q3 AAPL.pdf']).load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create an index from the documents\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "\n",
    "# Query engine for handling queries\n",
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from typing import Dict, List, Any\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class QueryMultiStepEvent(Event):\n",
    "    \"\"\"\n",
    "    Event containing results of a multi-step query process.\n",
    "\n",
    "    Attributes:\n",
    "        nodes (List[NodeWithScore]): Nodes with scores.\n",
    "        source_nodes (List[NodeWithScore]): Source nodes.\n",
    "        final_response_metadata (Dict[str, Any]): Metadata for the response.\n",
    "    \"\"\"\n",
    "    nodes: List[NodeWithScore]\n",
    "    source_nodes: List[NodeWithScore]\n",
    "    final_response_metadata: Dict[str, Any]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow, Context, StartEvent, StopEvent, step\n",
    ")\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "from llama_index.core.schema import QueryBundle, TextNode\n",
    "from typing import cast, Dict, Any\n",
    "\n",
    "\n",
    "class MultiStepQueryEngineWorkflow(Workflow):\n",
    "\n",
    "    def combine_queries(\n",
    "        self, query_bundle: QueryBundle, prev_reasoning: str, index_summary: str, llm\n",
    "    ) -> QueryBundle:\n",
    "        \"\"\"\n",
    "        Dynamically refine the query for the next step using LLM.\n",
    "\n",
    "        Parameters:\n",
    "        - query_bundle: Current query.\n",
    "        - prev_reasoning: All prior questions and answers.\n",
    "        - index_summary: High-level summary of the index data.\n",
    "        \"\"\"\n",
    "        prompt_template = (\n",
    "            \"You are solving a multi-step question step by step.\\n\"\n",
    "            \"Index Summary: {index_summary}\\n\\n\"\n",
    "            \"Previous Steps and Answers:\\n{prev_reasoning}\\n\\n\"\n",
    "            \"Current Query: {current_query}\\n\\n\"\n",
    "            \"Based on the above context, generate the next refined sub-question. \"\n",
    "            \"If no further refinement is needed, respond with 'None'.\\n\\n\"\n",
    "            \"Refined Query:\"\n",
    "        )\n",
    "\n",
    "        # Build prompt with current context\n",
    "        detailed_prompt = prompt_template.format(\n",
    "            index_summary=index_summary,\n",
    "            prev_reasoning=prev_reasoning.strip(),\n",
    "            current_query=query_bundle.query_str,\n",
    "        )\n",
    "\n",
    "        # Generate the next refined query using the LLM\n",
    "        refined_query = llm.complete(detailed_prompt).text.strip()\n",
    "\n",
    "        # Handle stopping condition\n",
    "        if refined_query.lower() in [\"none\", \"no query\", \"stop\"]:\n",
    "            refined_query = \"None\"\n",
    "\n",
    "        return QueryBundle(query_str=refined_query)\n",
    "\n",
    "\n",
    "\n",
    "    def default_stop_fn(self, stop_dict: Dict) -> bool:\n",
    "        query_bundle = cast(QueryBundle, stop_dict.get(\"query_bundle\"))\n",
    "        return \"none\" in query_bundle.query_str.lower()\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def query_multistep(self, ctx: Context, ev: StartEvent) -> QueryMultiStepEvent:\n",
    "        \"\"\"Execute the multi-step query process.\"\"\"\n",
    "        prev_reasoning, cur_steps = \"\", 0\n",
    "        should_stop = False\n",
    "\n",
    "        # Extract required inputs\n",
    "        num_steps = ev.get(\"num_steps\", 3)\n",
    "        query = ev.get(\"query\")\n",
    "        if not query:\n",
    "            raise ValueError(\"Query must be provided in StartEvent.\")\n",
    "        \n",
    "        index_summary = ev.get(\"index_summary\", \"\")\n",
    "        llm, query_engine = Settings.llm, ev.get(\"query_engine\")\n",
    "\n",
    "        # Initialize metadata and results\n",
    "        text_chunks, source_nodes = [], []\n",
    "        final_response_metadata: Dict[str, Any] = {\"sub_qa\": []}\n",
    "\n",
    "        # Set initial query into the context\n",
    "        await ctx.set(\"query\", query)\n",
    "\n",
    "        while not should_stop:\n",
    "            if cur_steps >= num_steps:\n",
    "                break\n",
    "\n",
    "            # Retrieve the query dynamically from context\n",
    "            query = await ctx.get(\"query\")\n",
    "\n",
    "            # Generate refined query\n",
    "            updated_query_bundle = self.combine_queries(\n",
    "                QueryBundle(query_str=query), prev_reasoning, index_summary, llm\n",
    "            )\n",
    "\n",
    "            # Check for stop condition\n",
    "            if \"none\" in updated_query_bundle.query_str.lower():\n",
    "                should_stop = True\n",
    "                break\n",
    "\n",
    "            print(f\"Step {cur_steps}: {updated_query_bundle.query_str}\")\n",
    "\n",
    "            # Execute the query and append results\n",
    "            cur_response = query_engine.query(updated_query_bundle)\n",
    "\n",
    "            # Update context and reasoning\n",
    "            await ctx.set(\"query\", updated_query_bundle.query_str)  # Set refined query back in context\n",
    "            cur_qa_text = f\"Q: {updated_query_bundle.query_str}\\nA: {cur_response}\"\n",
    "            text_chunks.append(cur_qa_text)\n",
    "            source_nodes.extend(cur_response.source_nodes)\n",
    "\n",
    "            # Update metadata\n",
    "            final_response_metadata[\"sub_qa\"].append((updated_query_bundle.query_str, cur_response))\n",
    "            prev_reasoning += f\"- Question: {updated_query_bundle.query_str}\\n- Answer: {cur_response}\\n\"\n",
    "            cur_steps += 1\n",
    "\n",
    "        # Prepare response event\n",
    "        nodes = [NodeWithScore(node=TextNode(text=text_chunk)) for text_chunk in text_chunks]\n",
    "        return QueryMultiStepEvent(nodes=nodes, source_nodes=source_nodes, final_response_metadata=final_response_metadata)\n",
    "\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def synthesize(self, ctx: Context, ev: QueryMultiStepEvent) -> StopEvent:\n",
    "        response_synthesizer = get_response_synthesizer()\n",
    "        query = await ctx.get(\"query\")\n",
    "        final_response = await response_synthesizer.asynthesize(\n",
    "            query=query, nodes=ev.nodes, additional_source_nodes=ev.source_nodes\n",
    "        )\n",
    "        final_response.metadata = ev.final_response_metadata\n",
    "        return StopEvent(result=final_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the workflow\n",
    "w = MultiStepQueryEngineWorkflow(timeout=200)\n",
    "\n",
    "async def main():\n",
    "    result = await w.run(\n",
    "        query=\"In which city did the author found his first company, Viaweb?\",\n",
    "        query_engine=query_engine,\n",
    "        index_summary=\"Used to answer questions about the author and Viaweb.\",\n",
    "        num_steps=5,\n",
    "    )\n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(result)\n",
    "    print(\"\\nStep-by-Step Reasoning:\")\n",
    "    sub_qa = result.metadata[\"sub_qa\"]\n",
    "    for idx, (question, answer) in enumerate(sub_qa):\n",
    "        print(f\"Step {idx}:\")\n",
    "        print(f\"  Question: {question}\")\n",
    "        print(f\"  Answer: {answer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Based on the provided context, a refined sub-question could be:\n",
      "\n",
      "In what year was the author's first company, Viaweb, founded?\n",
      "Step 1: Based on the current query, a refined sub-question could be:\n",
      "\n",
      "What is the approximate year when Paul Graham submitted his camera-ready copy of ANSI Common Lisp to the publishers?\n",
      "Step 2: Based on the provided context, a refined sub-question could be:\n",
      "\n",
      "In what year did Paul Graham start working on Lisp again?\n",
      "\n",
      "Final Result:\n",
      "2015.\n",
      "\n",
      "Step-by-Step Reasoning:\n",
      "Step 0:\n",
      "  Question: Based on the provided context, a refined sub-question could be:\n",
      "\n",
      "In what year was the author's first company, Viaweb, founded?\n",
      "  Answer: The year Viaweb was founded is not explicitly stated in the text. However, it can be inferred that Viaweb was founded after Paul Graham had submitted his camera-ready copy of ANSI Common Lisp to the publishers in the summer of 1995.\n",
      "Step 1:\n",
      "  Question: Based on the current query, a refined sub-question could be:\n",
      "\n",
      "What is the approximate year when Paul Graham submitted his camera-ready copy of ANSI Common Lisp to the publishers?\n",
      "  Answer: The text does not provide specific information about the exact year when Paul Graham submitted his camera-ready copy of ANSI Common Lisp to the publishers. However, it mentions that he started working on Lisp again in March 2015 and continued until he finished writing an essay for himself to answer a question about the minimum set of predefined operators needed to write an interpreter for a language in itself.\n",
      "Step 2:\n",
      "  Question: Based on the provided context, a refined sub-question could be:\n",
      "\n",
      "In what year did Paul Graham start working on Lisp again?\n",
      "  Answer: 2015.\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muthu\\AppData\\Local\\Temp\\ipykernel_45100\\4200408092.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) draw_all_possible_flows. (Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.)\n",
      "  draw_all_possible_flows(w, filename=\"workflow_multistep.html\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workflow_multistep.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w, filename=\"workflow_multistep.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
