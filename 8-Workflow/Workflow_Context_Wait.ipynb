{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Define custom events\n",
    "class JokeEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "class AnalysisRequestEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "class AnalysisResponseEvent(Event):\n",
    "    critique: str\n",
    "\n",
    "\n",
    "# Define the workflow\n",
    "class JokeFlow(Workflow):\n",
    "    llm=ollama_llm\n",
    "    # llm = OpenAI()\n",
    "\n",
    "    @step\n",
    "    async def generate_joke(self, ctx: Context, ev: StartEvent) -> JokeEvent:\n",
    "        \"\"\"\n",
    "        Generate a joke based on the provided topic and store the topic in the global context.\n",
    "        \"\"\"\n",
    "        # Extract the topic from the StartEvent\n",
    "        topic = ev.topic\n",
    "\n",
    "        # Store the topic in global context for later use\n",
    "        await ctx.set(\"topic\", topic)\n",
    "\n",
    "        # Generate a joke using the LLM\n",
    "        prompt = f\"Write your best joke about {topic}.\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "\n",
    "        # Return the generated joke as a JokeEvent\n",
    "        return JokeEvent(joke=str(response))\n",
    "\n",
    "    @step\n",
    "    async def request_analysis(self, ctx: Context, ev: JokeEvent) -> AnalysisRequestEvent:\n",
    "        \"\"\"\n",
    "        Request an analysis of the joke and store the joke in the global context.\n",
    "        \"\"\"\n",
    "        # Extract the joke from the JokeEvent\n",
    "        joke = ev.joke\n",
    "\n",
    "        # Store the joke in global context for later use\n",
    "        await ctx.set(\"joke\", joke)\n",
    "\n",
    "        # Return an AnalysisRequestEvent to trigger the analysis step\n",
    "        return AnalysisRequestEvent(joke=joke)\n",
    "\n",
    "    @step\n",
    "    async def critique_joke(self, ctx: Context, ev: AnalysisRequestEvent) -> AnalysisResponseEvent:\n",
    "        \"\"\"\n",
    "        Critique the joke and store the critique in the global context.\n",
    "        \"\"\"\n",
    "        # Extract the joke from the AnalysisRequestEvent\n",
    "        joke = ev.joke\n",
    "\n",
    "        # Retrieve the joke from the global context\n",
    "        #joke = await ctx.get(\"joke\")\n",
    "\n",
    "        # Generate a critique using the LLM\n",
    "        prompt = f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "\n",
    "        # Store the critique in the global context\n",
    "        await ctx.set(\"critique\", str(response))\n",
    "\n",
    "        # Return the critique as an AnalysisResponseEvent\n",
    "        return AnalysisResponseEvent(critique=str(response))\n",
    "\n",
    "    @step\n",
    "    async def finalize_result(\n",
    "        self, ctx: Context, ev: AnalysisResponseEvent | JokeEvent\n",
    "    ) -> StopEvent | None:\n",
    "        \"\"\"\n",
    "        Wait for both the JokeEvent and the AnalysisResponseEvent to finalize the result.\n",
    "        \"\"\"\n",
    "        # Wait for both the JokeEvent and AnalysisResponseEvent to arrive\n",
    "        data = ctx.collect_events(ev, [JokeEvent, AnalysisResponseEvent])\n",
    "\n",
    "        # If all events are not available yet, return None\n",
    "        if data is None:\n",
    "            return None\n",
    "\n",
    "        # Unpack the collected events\n",
    "        joke_event, analysis_response_event = data\n",
    "\n",
    "        # Finalize the result by combining the joke and its critique\n",
    "        result = f\"Joke: {joke_event.joke}\\n\\nCritique: {analysis_response_event.critique}\"\n",
    "\n",
    "        # Return the finalized result\n",
    "        return StopEvent(result=result)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the workflow\n",
    "w = JokeFlow(timeout=120, verbose=True)\n",
    "\n",
    "async def main():\n",
    "    # Start the workflow with a topic\n",
    "    result = await w.run(topic=\"pirates\")\n",
    "\n",
    "    # Print the final result\n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(str(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w, filename=\"joke_workflow_context.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
