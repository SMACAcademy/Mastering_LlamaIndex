{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Define custom events\n",
    "class JokeEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "class AnalysisRequestEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "class AnalysisResponseEvent(Event):\n",
    "    critique: str\n",
    "\n",
    "\n",
    "# Define the workflow\n",
    "class JokeFlow(Workflow):\n",
    "    llm=ollama_llm\n",
    "    # llm = OpenAI()\n",
    "\n",
    "    @step\n",
    "    async def generate_joke(self, ctx: Context, ev: StartEvent) -> JokeEvent:\n",
    "        \"\"\"\n",
    "        Generate a joke based on the provided topic and store the topic in the global context.\n",
    "        \"\"\"\n",
    "        # Extract the topic from the StartEvent\n",
    "        topic = ev.topic\n",
    "\n",
    "        # Store the topic in global context for later use\n",
    "        await ctx.set(\"topic\", topic)\n",
    "\n",
    "        # Generate a joke using the LLM\n",
    "        prompt = f\"Write your best joke about {topic}.\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "\n",
    "        # Return the generated joke as a JokeEvent\n",
    "        return JokeEvent(joke=str(response))\n",
    "\n",
    "    @step\n",
    "    async def request_analysis(self, ctx: Context, ev: JokeEvent) -> AnalysisRequestEvent:\n",
    "        \"\"\"\n",
    "        Request an analysis of the joke and store the joke in the global context.\n",
    "        \"\"\"\n",
    "        # Extract the joke from the JokeEvent\n",
    "        joke = ev.joke\n",
    "\n",
    "        # Store the joke in global context for later use\n",
    "        await ctx.set(\"joke\", joke)\n",
    "\n",
    "        # Return an AnalysisRequestEvent to trigger the analysis step\n",
    "        return AnalysisRequestEvent(joke=joke)\n",
    "\n",
    "    @step\n",
    "    async def critique_joke(self, ctx: Context, ev: AnalysisRequestEvent) -> AnalysisResponseEvent:\n",
    "        \"\"\"\n",
    "        Critique the joke and store the critique in the global context.\n",
    "        \"\"\"\n",
    "        # Extract the joke from the AnalysisRequestEvent\n",
    "        joke = ev.joke\n",
    "\n",
    "        # Retrieve the joke from the global context\n",
    "        #joke = await ctx.get(\"joke\")\n",
    "\n",
    "        # Generate a critique using the LLM\n",
    "        prompt = f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "\n",
    "        # Store the critique in the global context\n",
    "        await ctx.set(\"critique\", str(response))\n",
    "\n",
    "        # Return the critique as an AnalysisResponseEvent\n",
    "        return AnalysisResponseEvent(critique=str(response))\n",
    "\n",
    "    @step\n",
    "    async def finalize_result(\n",
    "        self, ctx: Context, ev: AnalysisResponseEvent | JokeEvent\n",
    "    ) -> StopEvent | None:\n",
    "        \"\"\"\n",
    "        Wait for both the JokeEvent and the AnalysisResponseEvent to finalize the result.\n",
    "        \"\"\"\n",
    "        # Wait for both the JokeEvent and AnalysisResponseEvent to arrive\n",
    "        data = ctx.collect_events(ev, [JokeEvent, AnalysisResponseEvent])\n",
    "\n",
    "        # If all events are not available yet, return None\n",
    "        if data is None:\n",
    "            return None\n",
    "\n",
    "        # Unpack the collected events\n",
    "        joke_event, analysis_response_event = data\n",
    "\n",
    "        # Finalize the result by combining the joke and its critique\n",
    "        result = f\"Joke: {joke_event.joke}\\n\\nCritique: {analysis_response_event.critique}\"\n",
    "\n",
    "        # Return the finalized result\n",
    "        return StopEvent(result=result)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the workflow\n",
    "w = JokeFlow(timeout=120, verbose=True)\n",
    "\n",
    "async def main():\n",
    "    # Start the workflow with a topic\n",
    "    result = await w.run(topic=\"pirates\")\n",
    "\n",
    "    # Print the final result\n",
    "    print(\"\\nFinal Result:\")\n",
    "    print(str(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step generate_joke\n",
      "Step generate_joke produced event JokeEvent\n",
      "Running step finalize_result\n",
      "Step finalize_result produced no event\n",
      "Running step request_analysis\n",
      "Step request_analysis produced event AnalysisRequestEvent\n",
      "Running step critique_joke\n",
      "Step critique_joke produced event AnalysisResponseEvent\n",
      "Running step finalize_result\n",
      "Step finalize_result produced event StopEvent\n",
      "\n",
      "Final Result:\n",
      "Joke: Why did the pirate quit his job?\n",
      "\n",
      "Because he was sick of all the arrrr-guments! (get it?)\n",
      "\n",
      "Critique: The joke in question is a play on words, using the nautical term \"arrrr\" to create a pun. Here's a thorough analysis and critique of the joke:\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "1. **Wordplay**: The use of \"arrrr\" as a verb, implying disagreement or argument, is a clever play on words. It's a common phrase associated with pirates, but in this context, it takes on a new meaning.\n",
      "2. **Simple and easy to understand**: The joke is straightforward and doesn't require complex setup or inside knowledge. This makes it accessible to a wide audience.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "1. **Predictability**: The punchline is somewhat predictable, as the wordplay is fairly obvious. This might reduce the surprise and delight of the joke.\n",
      "2. **Lack of originality**: The use of \"arrrr\" as a verb is not particularly new or innovative. While it's still effective, it's not a unique twist on traditional puns.\n",
      "3. **Limited humor scope**: The joke relies heavily on wordplay, which might not appeal to everyone. Some people might find the pun too cheesy or corny.\n",
      "\n",
      "**Critique:**\n",
      "\n",
      "While the joke is well-crafted and easy to understand, its predictability and lack of originality hold it back from being truly memorable or impactful. To improve the joke, consider adding more complexity or surprise to the punchline. For example:\n",
      "\n",
      "* Use a more unexpected wordplay, like \"arrrr-guments\" sounding similar to \"arguments,\" but with a pirate twist.\n",
      "* Add a layer of cleverness by using a double meaning or a play on expectations.\n",
      "* Experiment with different delivery styles, such as a deadpan or an over-the-top performance, to add more humor and personality.\n",
      "\n",
      "**Alternative versions:**\n",
      "\n",
      "To take the joke to the next level, consider these alternative versions:\n",
      "\n",
      "* Why did the pirate quit his job? Because he was fed up with all the sea-rious disagreements!\n",
      "* Why did the pirate quit his job? He was sick of all the hook-ing arguments!\n",
      "\n",
      "These revised jokes incorporate more clever wordplay and unexpected twists, making them more engaging and memorable.\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joke_workflow_context.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muthu\\AppData\\Local\\Temp\\ipykernel_15160\\87609920.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) draw_all_possible_flows. (Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.)\n",
      "  draw_all_possible_flows(w, filename=\"joke_workflow_context.html\")\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w, filename=\"joke_workflow_context.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
