{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-openai\n",
    "!pip install llama-index\n",
    "\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    #model=\"llama3.2:latest\",\n",
    "    model=\"mistral:7b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.4\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import download_llama_dataset\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# download and install dependencies for benchmark dataset\n",
    "rag_dataset, documents = download_llama_dataset(\n",
    "    \"EvaluatingLlmSurveyPaperDataset\", \"./data_survey_paper\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_dataset.to_pandas()[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_dataset_subset = rag_dataset.model_copy()\n",
    "rag_dataset_subset.examples = rag_dataset.examples[:15]  # Update with the first 40 examples\n",
    "\n",
    "\n",
    "# Run predictions on the subset\n",
    "prediction_dataset_subset = rag_dataset_subset.make_predictions_with(\n",
    "    predictor=query_engine, batch_size=10, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the rag_dataset_subset\n",
    "size = len(rag_dataset_subset.examples)\n",
    "\n",
    "print(f\"Size of rag_dataset_subset: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#prediction_dataset = await rag_dataset.amake_predictions_with(\n",
    "#    predictor=query_engine, batch_size=20, show_progress=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the gpt-4 judges\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    AnswerRelevancyEvaluator,\n",
    "    ContextRelevancyEvaluator,\n",
    ")\n",
    "\n",
    "judges = {}\n",
    "\n",
    "judges[\"answer_relevancy\"] = AnswerRelevancyEvaluator(\n",
    "    llm=ollama_llm,\n",
    ")\n",
    "\n",
    "judges[\"context_relevancy\"] = ContextRelevancyEvaluator(\n",
    "    llm=ollama_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tasks = []\n",
    "for example, prediction in zip(\n",
    "    rag_dataset_subset.examples, prediction_dataset_subset.predictions\n",
    "):\n",
    "    eval_tasks.append(\n",
    "        judges[\"answer_relevancy\"].evaluate(\n",
    "            query=example.query,\n",
    "            response=prediction.response,\n",
    "            sleep_time_in_seconds=1.0,\n",
    "        )\n",
    "    )\n",
    "    eval_tasks.append(\n",
    "        judges[\"context_relevancy\"].evaluate(\n",
    "            query=example.query,\n",
    "            contexts=prediction.contexts,\n",
    "            sleep_time_in_seconds=1.0,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results1 = tqdm_asyncio.gather(*eval_tasks[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results2 = tqdm_asyncio.gather(*eval_tasks[20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = eval_results1 + eval_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = {\n",
    "    \"answer_relevancy\": eval_results[::2],\n",
    "    \"context_relevancy\": eval_results[1::2],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "import pandas as pd\n",
    "\n",
    "deep_dfs = {}\n",
    "mean_dfs = {}\n",
    "for metric in evals.keys():\n",
    "    deep_df, mean_df = get_eval_results_df(\n",
    "        names=[\"baseline\"] * len(evals[metric]),\n",
    "        results_arr=evals[metric],\n",
    "        metric=metric,\n",
    "    )\n",
    "    deep_dfs[metric] = deep_df\n",
    "    mean_dfs[metric] = mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores_df = pd.concat(\n",
    "    [mdf.reset_index() for _, mdf in mean_dfs.items()],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "mean_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_dfs[\"answer_relevancy\"][\"scores\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_dfs[\"context_relevancy\"][\"scores\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayify_df(deep_dfs[\"context_relevancy\"].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = deep_dfs[\"context_relevancy\"][\"scores\"] < 1\n",
    "displayify_df(deep_dfs[\"context_relevancy\"][cond].head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
