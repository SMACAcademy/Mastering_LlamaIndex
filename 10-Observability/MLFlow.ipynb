{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mlflow>=2.15 llama-index>=0.10.44 -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Configure Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    #model=\"llama3.2:latest\",\n",
    "    model=\"mistral:7b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Configure embedding model\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0}\n",
    ")\n",
    "\n",
    "Settings.llm = ollama_llm\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Example Document used to Enrich LLM Context -------------\n",
      "Doc ID: 940539c0-d8b0-4769-a5ba-075b6e24bfa1\n",
      "Text: Context LLMs are a phenomenal piece of technology for knowledge\n",
      "generation and reasoning. They are pre-trained on large amounts of\n",
      "publicly available data. How do we best augment LLMs with our own\n",
      "private data? We need a comprehensive toolkit to help perform this\n",
      "data augmentation for LLMs.  Proposed Solution That's where LlamaIndex\n",
      "comes in. Ll...\n",
      "\n",
      "------------- Example Query Engine -------------\n",
      " LlamaIndex is a data framework designed to help build applications utilizing Language Learning Models (LLMs). It offers tools for ingesting various data sources, structuring data, providing an advanced retrieval/query interface over the data, and integrating with outer application frameworks. It caters to both beginner and advanced users, offering a high-level API for easy usage and a lower-level API for customization and extension.\n",
      "\n",
      "------------- Example Retriever   -------------\n",
      "[NodeWithScore(node=TextNode(id_='74d1d297-4e7e-498c-9c1d-62f1bb998583', embedding=None, metadata={'filename': 'README.md', 'category': 'codebase'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='940539c0-d8b0-4769-a5ba-075b6e24bfa1', node_type='4', metadata={'filename': 'README.md', 'category': 'codebase'}, hash='5c48c73fe568636076bbdc5065d010a52d75dbf95cdb387b753b9e0fbaf7787a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Context\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\nThey are pre-trained on large amounts of publicly available data.\\nHow do we best augment LLMs with our own private data?\\nWe need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\nProposed Solution\\nThat\\'s where LlamaIndex comes in. LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps. It provides the following tools:\\n\\nOffers data connectors to ingest your existing data sources and data formats\\n(APIs, PDFs, docs, SQL, etc.)\\nProvides ways to structure your data (indices, graphs) so that this data can be\\neasily used with LLMs.\\nProvides an advanced retrieval/query interface over your data:\\nFeed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\\nAllows easy integrations with your outer application framework\\n(e.g. with LangChain, Flask, Docker, ChatGPT, anything else).\\nLlamaIndex provides tools for both beginner users and advanced users.\\nOur high-level API allows beginner users to use LlamaIndex to ingest and\\nquery their data in 5 lines of code. Our lower-level APIs allow advanced users to\\ncustomize and extend any module (data connectors, indices, retrievers, query engines,\\nreranking modules), to fit their needs.', mimetype='text/plain', start_char_idx=0, end_char_idx=1278, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6511635452370143)]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"------------- Example Document used to Enrich LLM Context -------------\"\n",
    ")\n",
    "llama_index_example_document = Document.example()\n",
    "print(llama_index_example_document)\n",
    "index = VectorStoreIndex.from_documents([llama_index_example_document])\n",
    "\n",
    "print(\"\\n------------- Example Query Engine -------------\")\n",
    "query_response = index.as_query_engine().query(\"What is llama_index?\")\n",
    "print(query_response)\n",
    "\n",
    "print(\"\\n------------- Example Retriever   -------------\")\n",
    "retriever_response = index.as_retriever().retrieve(\"What is llama_index?\")\n",
    "print(retriever_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/23 15:39:48 INFO mlflow.llama_index.serialize_objects: API key(s) will be removed from the global Settings object during serialization to protect against key leakage. At inference time, the key(s) must be passed as environment variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdfe461455346a1938f919eb0497dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique identifier for the model location for loading: runs:/b3d9992b0c144dd3848463a7c1b7b73f/llama_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'my_llama_index_vector_store' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'my_llama_index_vector_store'.\n"
     ]
    }
   ],
   "source": [
    "mlflow.llama_index.autolog()  # This is for enabling tracing\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.llama_index.log_model(\n",
    "        index,\n",
    "        artifact_path=\"llama_index\",\n",
    "        engine_type=\"query\",  # Defines the pyfunc and spark_udf inference type\n",
    "        input_example=\"hi\",  # Infers signature\n",
    "        registered_model_name=\"my_llama_index_vector_store\",  # Stores an instance in the model registry\n",
    "    )\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "    model_uri = f\"runs:/{run_id}/llama_index\"\n",
    "    print(f\"Unique identifier for the model location for loading: {model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------- Inference via Llama Index   -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/23 15:42:44 WARNING mlflow.tracing.processor.mlflow: Creating a trace within the default experiment with id '0'. It is strongly recommended to not use the default experiment to log traces due to ambiguous search results and probable performance issues over time due to directory table listing performance degradation with high volumes of directories within a specific path. To avoid performance and disambiguation issues, set the experiment for your environment using `mlflow.set_experiment()` API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! LlamaIndex is a data framework designed to help you build applications using Language Learning Models (LLMs). It offers tools such as data connectors for various sources like APIs, PDFs, docs, SQL, and more. Additionally, it provides ways to structure your data for easy use with LLMs and an advanced retrieval/query interface. This allows you to feed in any LLM input prompt and receive retrieved context and knowledge-augmented output. It also offers easy integrations with various application frameworks like LangChain, Flask, Docker, and more. LlamaIndex caters to both beginner and advanced users, offering a high-level API for quick usage and lower-level APIs for customization and extension.\n",
      "\n",
      "------------- Inference via MLflow PyFunc -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/23 15:42:51 WARNING mlflow.tracing.processor.mlflow: Creating a trace within the default experiment with id '0'. It is strongly recommended to not use the default experiment to log traces due to ambiguous search results and probable performance issues over time due to directory table listing performance degradation with high volumes of directories within a specific path. To avoid performance and disambiguation issues, set the experiment for your environment using `mlflow.set_experiment()` API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! LlamaIndex is a data framework designed to help you build applications using Language Learning Models (LLMs). It offers tools such as data connectors for various sources like APIs, PDFs, docs, SQL, etc., ways to structure your data, an advanced retrieval/query interface, and easy integrations with other application frameworks. For beginner users, it provides a high-level API that allows you to ingest and query your data in just 5 lines of code. Advanced users can customize and extend its modules for their specific needs.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------------- Inference via Llama Index   -------------\")\n",
    "index = mlflow.llama_index.load_model(model_uri)\n",
    "query_response = index.as_query_engine().query(\"hi\")\n",
    "print(query_response)\n",
    "\n",
    "print(\"\\n------------- Inference via MLflow PyFunc -------------\")\n",
    "index = mlflow.pyfunc.load_model(model_uri)\n",
    "query_response = index.predict(\"hi\")\n",
    "print(query_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Start the MLflow UI in a background process\n",
    "mlflow_ui_command = [\"mlflow\", \"ui\", \"--port\", \"5000\"]\n",
    "\n",
    "# Use subprocess.Popen without preexec_fn for Windows compatibility\n",
    "process = subprocess.Popen(\n",
    "    mlflow_ui_command,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    creationflags=subprocess.CREATE_NEW_PROCESS_GROUP,  # Windows equivalent for new process group\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this in linux based system\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Start the MLflow UI in a background process\n",
    "mlflow_ui_command = [\"mlflow\", \"ui\", \"--port\", \"5000\"]\n",
    "subprocess.Popen(\n",
    "    mlflow_ui_command,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    preexec_fn=os.setsid,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
