{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-llms-openai\n",
    "!pip install llama-index\n",
    "!pip install llama-index-embeddings-huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser, SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Configure Ollama LLM and Embeddings\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\", max_length=512\n",
    ")\n",
    "\n",
    "# Set global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sentence Window Node Parser\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")\n",
    "\n",
    "# Set a base text splitter for default indexing\n",
    "text_splitter = SentenceSplitter()\n",
    "Settings.text_splitter = text_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample document\n",
    "!curl -o IPCC_AR6_WGII_Chapter03.pdf https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Load the document\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[\"../data_ipcc/IPCC_AR6_WGII_Chapter03.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract nodes using Sentence Window Parser\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# Extract base nodes with default text splitting\n",
    "base_nodes = text_splitter.get_nodes_from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nodes))\n",
    "print(len(base_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create indexes\n",
    "sentence_index = VectorStoreIndex(nodes)\n",
    "base_index = VectorStoreIndex(base_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Engine with Metadata Replacement PostProcessor\n",
    "query_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Execute query\n",
    "response = query_engine.query(\"What are the concerns surrounding the AMOC?\")\n",
    "print(response)\n",
    "\n",
    "# Extract context window and original sentence\n",
    "window = response.source_nodes[0].node.metadata[\"window\"]\n",
    "original_sentence = response.source_nodes[0].node.metadata[\"original_text\"]\n",
    "\n",
    "print(f\"Window: {window}\")\n",
    "print(\"------------------\")\n",
    "print(f\"Original Sentence: {original_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default query engine\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=2)\n",
    "\n",
    "# Execute query\n",
    "base_response = base_query_engine.query(\"What are the concerns surrounding the AMOC?\")\n",
    "print(base_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare retrieved nodes for both methods\n",
    "print(\"Sentence Window Method:\")\n",
    "for source_node in response.source_nodes:\n",
    "    print(source_node.node.metadata[\"original_text\"])\n",
    "    print(\"--------\")\n",
    "\n",
    "print(\"Base Index Method:\")\n",
    "for source_node in base_response.source_nodes:\n",
    "    print(\"AMOC mentioned?\", \"AMOC\" in source_node.node.text)\n",
    "    print(\"--------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
